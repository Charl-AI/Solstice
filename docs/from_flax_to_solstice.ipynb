{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Flax to Solstice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: turn into Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook starts with an MNIST classification project and demonstrates how to incrementally buy in to Solstice in 3 steps: \n",
    "\n",
    "1. Organise training code with `solstice.Experiment`\n",
    "2. Implement `solstice.Metrics` for tracking metrics\n",
    "3. Use the premade `solstice.train()` loop with `solstice.Callback`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST in pure Flax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# stop tensorflow grabbing GPU memory\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "train_ds = tfds.load(name=\"mnist\", split=\"train\", as_supervised=True, data_dir=\"/tmp/data\")\n",
    "assert isinstance(train_ds, tf.data.Dataset)\n",
    "preprocess_mnist = lambda x, y: (\n",
    "    tf.reshape(tf.cast(x, tf.float32) / 255, (784,)),\n",
    "    tf.cast(y, tf.float32),\n",
    ")\n",
    "train_ds = train_ds.map(preprocess_mnist).batch(32).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the Flax model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Any\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    dtype: Any = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for i, feat in enumerate(self.features):\n",
    "            x = nn.Dense(feat, dtype=self.dtype)(x)\n",
    "            if i != len(self.features) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a `TrainState` object and training step (notice how this is already quite similar to `solstice.Experiment`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "import jax\n",
    "import optax\n",
    "import dataclasses\n",
    "from flax import struct\n",
    "\n",
    "@struct.dataclass\n",
    "class TrainState:\n",
    "    params: optax.Params\n",
    "    opt_state: optax.OptState\n",
    "    tx: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, rng: int, learning_rate: float):\n",
    "        key = jax.random.PRNGKey(rng)\n",
    "        model = MLP(features=[300, 300, 10])\n",
    "        params = model.init(key, jnp.ones([1, 784]))['params']\n",
    "        tx = optax.sgd(learning_rate)\n",
    "        opt_state = tx.init(params)\n",
    "        return cls(params, opt_state, tx, model.apply)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    state: TrainState, batch: Tuple[jnp.ndarray, jnp.ndarray]\n",
    "    ) -> Tuple[TrainState, Any]:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, imgs)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.one_hot(labels, 10)))\n",
    "        return loss, logits\n",
    "\n",
    "    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    updates, new_opt_state = state.tx.update(grads, state.opt_state, state.params)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "  \n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(preds == labels)\n",
    "    metrics = {'accuracy': accuracy, 'loss': loss}\n",
    "    return dataclasses.replace(state, params=new_params, opt_state=new_opt_state), metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make a training loop and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:03<00:00, 573.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, metrics={'accuracy': DeviceArray(0.9185667, dtype=float32), 'loss': DeviceArray(0.27219555, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1134.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, metrics={'accuracy': DeviceArray(0.96816665, dtype=float32), 'loss': DeviceArray(0.10662512, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1252.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, metrics={'accuracy': DeviceArray(0.97900003, dtype=float32), 'loss': DeviceArray(0.07093416, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def flax_train(state: TrainState, train_ds: tf.data.Dataset, num_epochs: int):\n",
    "    metrics = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm(train_ds.as_numpy_iterator(), total=len(train_ds)):\n",
    "            state, batch_metrics = train_step(state, batch)\n",
    "            metrics.append(batch_metrics)\n",
    "        metrics = jax.tree_util.tree_map(lambda *ms: jnp.mean(jnp.array(ms)), *metrics)\n",
    "        print(f\"Epoch {epoch}, {metrics=}\")\n",
    "        metrics = []\n",
    "    return state\n",
    "\n",
    "state = TrainState.create(rng=0, learning_rate=0.1)\n",
    "trained_state = flax_train(state, train_ds, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `solstice.Experiment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we introduce the `solstice.Experiment`, a better way to organise your deep learning code. When converting from Flax to Solstice, notice that a couple of things happened:\n",
    "\n",
    "- We replaced `TrainState` with `solstice.Experiment`, using `__init__` instead of `.create`\n",
    "- We encapsulated the `train_step()` function into a `train_step()` method.\n",
    "- All mentions of `state` became mentions of `self`.\n",
    "- Instad of specifying fields as static up-front, we filter them in the JIT decorator (see the [Solstice Primer](https://charl-ai.github.io/Solstice/primer/) for more info).\n",
    "\n",
    "Notice that `self` is just a PyTree, and the `train_step` method is still a pure function. Like `TrainState`, `Experiment`s are immutable, so all updates are performed out-of-place by returning a new `Experiment` from the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:02<00:00, 842.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, metrics={'accuracy': DeviceArray(0.9185667, dtype=float32), 'loss': DeviceArray(0.27219555, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1027.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, metrics={'accuracy': DeviceArray(0.96816665, dtype=float32), 'loss': DeviceArray(0.10662512, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 979.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, metrics={'accuracy': DeviceArray(0.97900003, dtype=float32), 'loss': DeviceArray(0.07093416, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "import solstice\n",
    "\n",
    "class MNISTClassifier(solstice.Experiment):\n",
    "    params: optax.Params\n",
    "    opt_state: optax.OptState\n",
    "    tx: optax.GradientTransformation\n",
    "    apply_fn: Callable\n",
    "\n",
    "    def __init__(self, rng: int, learning_rate: float):\n",
    "        key = jax.random.PRNGKey(rng)\n",
    "        model = MLP(features=[300, 300, 10])\n",
    "        self.params = model.init(key, jnp.ones([1, 784]))['params']\n",
    "        self.tx = optax.sgd(learning_rate)\n",
    "        self.opt_state = self.tx.init(self.params)\n",
    "        self.apply_fn = model.apply\n",
    "\n",
    "    @eqx.filter_jit(kwargs=dict(batch=True))\n",
    "    def train_step(self, batch: Tuple[jnp.ndarray, jnp.ndarray]) -> Tuple[\"MNISTClassifier\", Any]:\n",
    "        imgs, labels = batch\n",
    "\n",
    "        def loss_fn(params):\n",
    "            logits = self.apply_fn({'params': params}, imgs)\n",
    "            loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.one_hot(labels, 10)))\n",
    "            return loss, logits\n",
    "\n",
    "        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(self.params)\n",
    "        updates, new_opt_state = self.tx.update(grads, self.opt_state, self.params)\n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "    \n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        accuracy = jnp.mean(preds == labels)\n",
    "        metrics = {'accuracy': accuracy, 'loss': loss}\n",
    "        return solstice.replace(self, params=new_params, opt_state=new_opt_state), metrics\n",
    "\n",
    "    def eval_step(self, batch):\n",
    "        raise NotImplementedError(\"not bothering with eval in this example\")\n",
    "\n",
    "\n",
    "def solstice_train(exp: solstice.Experiment, train_ds: tf.data.Dataset, num_epochs: int):\n",
    "    metrics = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm(train_ds.as_numpy_iterator(), total=len(train_ds)):\n",
    "            exp, batch_metrics = exp.train_step(batch)\n",
    "            metrics.append(batch_metrics)\n",
    "        metrics = jax.tree_util.tree_map(lambda *ms: jnp.mean(jnp.array(ms)), *metrics)\n",
    "        print(f\"Epoch {epoch}, {metrics=}\")\n",
    "        metrics = []\n",
    "    return exp\n",
    "\n",
    "exp = MNISTClassifier(rng=0, learning_rate=0.1)\n",
    "trained_exp = solstice_train(exp, train_ds, num_epochs=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that none of the logic has changed (in fact all the computations and results are identical), just the organisation. Even without the rest of Solstice, this has a few advantages over the pure Flax code:\n",
    "\n",
    "- Better ergonomics due to creating experiments with `__init__` instead of custom classmethods.\n",
    "- Explicitly keeping related training code together in one place.\n",
    "- The flax code had implicit coupling between the `train_step()` and `TrainState`, it is now encapsulated into one class to make the dependency explicit.\n",
    "- It is now easier to define different `Experiment` classes for different experiments and sweep across them with with your favourite tools (such as hydra or wandb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `solstice.Metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the subtle gotcha in the metrics calculation above? The dataset size needed to be perfectly divisible by the batch size, otherwise the last batch would have had a different size so averaging the loss and accuracy over all batches would have been wrong. Accumulating and calculating metrics gets even harder when you are using metrics that are not 'averageable' such as precision. We provide `solstice.Metrics`, an API for keeping track of metrics scalably and without these headaches.\n",
    "\n",
    "A `solstice.Metrics` object knows how to do three things:\n",
    "- Calculate intermediate results from model outputs with `__init__`.\n",
    "- Accumulate results with other `solstice.Metrics` objects with `merge()`.\n",
    "- Calculate final metrics with `compute()`.\n",
    "\n",
    "Below, we integrate this into our current MNIST experiment, notice that the results are still the same, but the code is cleaner and more extensible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:03<00:00, 512.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, metrics={'accuracy': DeviceArray(0.9185667, dtype=float32), 'average_loss': DeviceArray(0.27219722, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:02<00:00, 655.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, metrics={'accuracy': DeviceArray(0.96816665, dtype=float32), 'average_loss': DeviceArray(0.10662578, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:02<00:00, 665.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, metrics={'accuracy': DeviceArray(0.97900003, dtype=float32), 'average_loss': DeviceArray(0.07093462, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "class MyMetrics(solstice.Metrics):\n",
    "    \"\"\"Custom Metrics class for calculating accuracy and average loss. Included for\n",
    "    didactic purposes, in practice `solstice.ClassificationMetrics` is better.\"\"\"\n",
    "\n",
    "    average_loss: float\n",
    "    count: int  # number of samples seen\n",
    "    num_correct: int\n",
    "\n",
    "    def __init__(self, preds: jnp.ndarray, targets: jnp.ndarray, loss: float) -> None:\n",
    "        self.average_loss = loss\n",
    "        self.count = preds.shape[0]  # assumes batch is first dim\n",
    "        self.num_correct = jnp.sum(preds == targets)\n",
    "\n",
    "    def merge(self, other: \"MyMetrics\") -> \"MyMetrics\":\n",
    "        # can simply sum num_correct and count\n",
    "        new_num_correct = self.num_correct + other.num_correct\n",
    "        new_count = self.count + other.count\n",
    "\n",
    "        # average loss is weighted by count from each object\n",
    "        new_loss = (\n",
    "            self.average_loss * self.count + other.average_loss * other.count\n",
    "        ) / (self.count + other.count)\n",
    "\n",
    "        return solstice.replace(\n",
    "            self, num_correct=new_num_correct, count=new_count, average_loss=new_loss\n",
    "        )\n",
    "\n",
    "    def compute(self) -> Mapping[str, float]:\n",
    "        return {\n",
    "            \"accuracy\": self.num_correct / self.count,\n",
    "            \"average_loss\": self.average_loss,\n",
    "        }\n",
    "\n",
    "class MNISTClassifierWithMetrics(solstice.Experiment):\n",
    "    params: optax.Params\n",
    "    opt_state: optax.OptState\n",
    "    tx: optax.GradientTransformation\n",
    "    apply_fn: Callable\n",
    "\n",
    "    def __init__(self, rng: int, learning_rate: float):\n",
    "        key = jax.random.PRNGKey(rng)\n",
    "        model = MLP(features=[300, 300, 10])\n",
    "        self.params = model.init(key, jnp.ones([1, 784]))['params']\n",
    "        self.tx = optax.sgd(learning_rate)\n",
    "        self.opt_state = self.tx.init(self.params)\n",
    "        self.apply_fn = model.apply\n",
    "\n",
    "    @eqx.filter_jit(kwargs=dict(batch=True))\n",
    "    def train_step(self, batch: Tuple[jnp.ndarray, jnp.ndarray]) -> Tuple[\"MNISTClassifierWithMetrics\", solstice.Metrics]:\n",
    "        imgs, labels = batch\n",
    "\n",
    "        def loss_fn(params):\n",
    "            logits = self.apply_fn({'params': params}, imgs)\n",
    "            loss = jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.one_hot(labels, 10)))\n",
    "            return loss, logits\n",
    "\n",
    "        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(self.params)\n",
    "        updates, new_opt_state = self.tx.update(grads, self.opt_state, self.params)\n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "    \n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        metrics = MyMetrics(preds, labels, loss)\n",
    "        return solstice.replace(self, params=new_params, opt_state=new_opt_state), metrics\n",
    "\n",
    "    def eval_step(self, batch):\n",
    "        raise NotImplementedError(\"not bothering with eval in this example\")\n",
    "\n",
    "\n",
    "def solstice_train_with_metrics(exp: solstice.Experiment, train_ds: tf.data.Dataset, num_epochs: int):\n",
    "    metrics = None\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm(train_ds.as_numpy_iterator(), total=len(train_ds)):\n",
    "            exp, batch_metrics = exp.train_step(batch)\n",
    "            metrics = batch_metrics if metrics is None else batch_metrics.merge(metrics)\n",
    "        assert metrics is not None\n",
    "        metrics = metrics.compute()\n",
    "        print(f\"Epoch {epoch}, {metrics=}\")\n",
    "        metrics = None\n",
    "    return exp\n",
    "\n",
    "\n",
    "exp = MNISTClassifierWithMetrics(rng=0, learning_rate=0.1)\n",
    "trained_exp = solstice_train_with_metrics(exp, train_ds, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solstice also provides some pre-made metrics classes, such as `solstice.ClassificationMetrics` for common use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `solstice.train()` and `solstice.Callback`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, training loops are boilerplate code. In general, they tend to have two parts: the loops that advance the training state, and the bits that make side effects such as logging and checkpointing work. Solstice comes with `solstice.train()`, a standard training loop which integrates with a flexible callback system for injecting side effects.\n",
    "\n",
    "Below, we use the built-in `solstice.LoggingCallback` with `solstice.train()` to cut down on boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3 [00:00<?, ?epoch/s]INFO:solstice:train step 0: {'accuracy': DeviceArray(0.9185667, dtype=float32), 'average_loss': DeviceArray(0.27219722, dtype=float32)}\n",
      "Training:  33%|███▎      | 1/3 [00:03<00:06,  3.26s/epoch]INFO:solstice:train step 1: {'accuracy': DeviceArray(0.96816665, dtype=float32), 'average_loss': DeviceArray(0.10662578, dtype=float32)}\n",
      "Training:  67%|██████▋   | 2/3 [00:06<00:03,  3.24s/epoch]INFO:solstice:train step 2: {'accuracy': DeviceArray(0.97900003, dtype=float32), 'average_loss': DeviceArray(0.07093462, dtype=float32)}\n",
      "Training: 100%|██████████| 3/3 [00:09<00:00,  3.23s/epoch]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"solstice\").setLevel(logging.INFO)\n",
    "\n",
    "# by default, `solstice.LoggingCallback` logs to the built-in Python logging system\n",
    "# with name 'solstice' and level INFO. You can also use this callback with TensorBoard etc...\n",
    "logging_callback = solstice.LoggingCallback()\n",
    "\n",
    "exp = MNISTClassifierWithMetrics(rng=0, learning_rate=0.1)\n",
    "trained_exp = solstice.train(exp, train_ds=train_ds, num_epochs=3, callbacks=[logging_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the results are still identical to the ones from the initial Flax code. All Solstice does is provide user-facing utilities for creating and scaling deep learning experiments in JAX. We encourage people to create their own `Callback`s to do more interesting things."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
