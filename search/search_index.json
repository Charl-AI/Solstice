{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solstice \u00a4 Solstice is a library for constructing modular and structured deep learning experiments in JAX. Built with Equinox , but designed for full interoparability with JAX neural network libraries e.g. Stax, Haiku, Flax, Optax etc... Why use Solstice in a world with Flax/Haiku/Objax/...? Solstice is not a neural network framework. It is a system for organising JAX code, with a small library of sane defaults for common use cases (think PyTorch Lightning , but for JAX). The library itself is simple and flexible, leaving most important decisions to the user - we aim to provide high-quality examples to demonstrate the different ways you can use this flexibility. Solstice is in the alpha stage of development, there may be API changes until we settle on a stable version 1.0.0 Installation \u00a4 First, install JAX , then: pip install solstice-jax Docs \u00a4 Solstice is fully documented, including a full API Reference, as well as tutorials and examples. Below, we provide a bare minimum example for how to get started. Getting Started \u00a4 The central abstraction in Solstice is the solstice.Experiment . An Experiment is a container for all functions and stateful objects that are relevant to a run. You can create an Experiment by subclassing solstice.Experiment and implementing the abstractmethods for initialisation, training, and evaluation. Experiments are best used with solstice.Metrics for tracking metrics and solstice.train() so you can stop writing boilerplate training loops. from typing import Any , Tuple import logging import jax import jax.numpy as jnp import solstice import tensorflow_datasets as tfds logging . basicConfig ( level = logging . INFO ) class RandomClassifier ( solstice . Experiment ): \"\"\"A terrible, terrible classifier for binary class problems :(\"\"\" rng_state : Any def __init__ ( self , rng : int ): self . rng_state = jax . random . PRNGKey ( rng ) def __call__ ( self , x ): del x return jax . random . bernoulli ( self . rng_state , p = 0.5 ) . astype ( jnp . float32 ) @jax . jit def train_step ( self , batch : Tuple [ jnp . ndarray , ... ] ) -> Tuple [ \"RandomClassifier\" , solstice . Metrics ]: x , y = batch preds = jax . vmap ( self )( x ) # use solstice Metrics API for convenient metrics calculation metrics = solstice . ClassificationMetrics ( preds , y , loss = jnp . nan , num_classes = 2 ) new_rng_state = jax . random . split ( self . rng_state )[ 0 ] return solstice . replace ( self , rng_state = new_rng_state ), metrics @jax . jit def eval_step ( self , batch : Tuple [ jnp . ndarray , ... ] ) -> Tuple [ \"RandomClassifier\" , solstice . Metrics ]: x , y = batch preds = jax . vmap ( self )( x ) metrics = solstice . ClassificationMetrics ( preds , y , loss = jnp . nan , num_classes = 2 ) return self , metrics train_ds = tfds . load ( name = \"mnist\" , split = \"train\" , as_supervised = True ) # type: Any train_ds = train_ds . batch ( 32 ) . prefetch ( 1 ) exp = RandomClassifier ( 42 ) # use solstice.train() with callbacks to remove boilerplate code trained_exp = solstice . train ( exp , num_epochs = 1 , train_ds = train_ds , callbacks = [ solstice . LoggingCallback ()] ) Notice that we were able to use pure JAX transformations such as jax.jit and jax.vmap within the class. This is because solstice.Experiment is just a subclass of Equinox.Module . We explain this further in the Solstice Primer , but in general, if you understand JAX/Equinox, you will understand Solstice. Incrementally buying-in \u00a4 Solstice is a library, not a framework, and it is important to us that you have the freedom to use as little or as much of it as you like. If are interested in starting using Solstice, but don't know where to begin, here are three steps towards Solstice-ification. Stage 1: organise your training code with solstice.Experiment \u00a4 The Experiment object contains stateful objects such as model and optimizer parameters and also encapsulates the steps for training and evaluation. In Flax, this would replace the TrainState object and serve to better organise your code. At this stage, the main advantage is that your code is more readable and scalable because you can define different Experiment s for different use cases. Stage 2: implement solstice.Metrics for tracking metrics \u00a4 A solstice.Metrics object knows how to calculate and accumulate intermediate results, before computing final metrics. The main advantage is the ability to scalably track lots of metrics with a common interface. By tracking intermediate results and computing at the end, it is easier to handle metrics which are not 'averageable' over batches (e.g. precision). Stage 3: use the premade solstice.train() loop with solstice.Callback s \u00a4 Training loops are usually boilerplate code. We provide premade training and testing loops which integrate with a simple and flexible callback system. This allows you to separate the basic logic of training from customisable side effects such as logging and checkpointing. We provide some useful pre-made callbacks and give examples for how to write your own. Our Logos \u00a4 We have two Solstice logos: the Summer Solstice and the Winter Solstice . Both were created with Dall-E mini (free license) with the following prompt: a logo featuring stonehenge during a solstice","title":"Getting Started"},{"location":"#solstice","text":"Solstice is a library for constructing modular and structured deep learning experiments in JAX. Built with Equinox , but designed for full interoparability with JAX neural network libraries e.g. Stax, Haiku, Flax, Optax etc... Why use Solstice in a world with Flax/Haiku/Objax/...? Solstice is not a neural network framework. It is a system for organising JAX code, with a small library of sane defaults for common use cases (think PyTorch Lightning , but for JAX). The library itself is simple and flexible, leaving most important decisions to the user - we aim to provide high-quality examples to demonstrate the different ways you can use this flexibility. Solstice is in the alpha stage of development, there may be API changes until we settle on a stable version 1.0.0","title":"Solstice"},{"location":"#installation","text":"First, install JAX , then: pip install solstice-jax","title":"Installation"},{"location":"#docs","text":"Solstice is fully documented, including a full API Reference, as well as tutorials and examples. Below, we provide a bare minimum example for how to get started.","title":"Docs"},{"location":"#getting-started","text":"The central abstraction in Solstice is the solstice.Experiment . An Experiment is a container for all functions and stateful objects that are relevant to a run. You can create an Experiment by subclassing solstice.Experiment and implementing the abstractmethods for initialisation, training, and evaluation. Experiments are best used with solstice.Metrics for tracking metrics and solstice.train() so you can stop writing boilerplate training loops. from typing import Any , Tuple import logging import jax import jax.numpy as jnp import solstice import tensorflow_datasets as tfds logging . basicConfig ( level = logging . INFO ) class RandomClassifier ( solstice . Experiment ): \"\"\"A terrible, terrible classifier for binary class problems :(\"\"\" rng_state : Any def __init__ ( self , rng : int ): self . rng_state = jax . random . PRNGKey ( rng ) def __call__ ( self , x ): del x return jax . random . bernoulli ( self . rng_state , p = 0.5 ) . astype ( jnp . float32 ) @jax . jit def train_step ( self , batch : Tuple [ jnp . ndarray , ... ] ) -> Tuple [ \"RandomClassifier\" , solstice . Metrics ]: x , y = batch preds = jax . vmap ( self )( x ) # use solstice Metrics API for convenient metrics calculation metrics = solstice . ClassificationMetrics ( preds , y , loss = jnp . nan , num_classes = 2 ) new_rng_state = jax . random . split ( self . rng_state )[ 0 ] return solstice . replace ( self , rng_state = new_rng_state ), metrics @jax . jit def eval_step ( self , batch : Tuple [ jnp . ndarray , ... ] ) -> Tuple [ \"RandomClassifier\" , solstice . Metrics ]: x , y = batch preds = jax . vmap ( self )( x ) metrics = solstice . ClassificationMetrics ( preds , y , loss = jnp . nan , num_classes = 2 ) return self , metrics train_ds = tfds . load ( name = \"mnist\" , split = \"train\" , as_supervised = True ) # type: Any train_ds = train_ds . batch ( 32 ) . prefetch ( 1 ) exp = RandomClassifier ( 42 ) # use solstice.train() with callbacks to remove boilerplate code trained_exp = solstice . train ( exp , num_epochs = 1 , train_ds = train_ds , callbacks = [ solstice . LoggingCallback ()] ) Notice that we were able to use pure JAX transformations such as jax.jit and jax.vmap within the class. This is because solstice.Experiment is just a subclass of Equinox.Module . We explain this further in the Solstice Primer , but in general, if you understand JAX/Equinox, you will understand Solstice.","title":"Getting Started"},{"location":"#incrementally-buying-in","text":"Solstice is a library, not a framework, and it is important to us that you have the freedom to use as little or as much of it as you like. If are interested in starting using Solstice, but don't know where to begin, here are three steps towards Solstice-ification.","title":"Incrementally buying-in"},{"location":"#stage-1-organise-your-training-code-with-solsticeexperiment","text":"The Experiment object contains stateful objects such as model and optimizer parameters and also encapsulates the steps for training and evaluation. In Flax, this would replace the TrainState object and serve to better organise your code. At this stage, the main advantage is that your code is more readable and scalable because you can define different Experiment s for different use cases.","title":"Stage 1: organise your training code with solstice.Experiment"},{"location":"#stage-2-implement-solsticemetrics-for-tracking-metrics","text":"A solstice.Metrics object knows how to calculate and accumulate intermediate results, before computing final metrics. The main advantage is the ability to scalably track lots of metrics with a common interface. By tracking intermediate results and computing at the end, it is easier to handle metrics which are not 'averageable' over batches (e.g. precision).","title":"Stage 2: implement solstice.Metrics for tracking metrics"},{"location":"#stage-3-use-the-premade-solsticetrain-loop-with-solsticecallbacks","text":"Training loops are usually boilerplate code. We provide premade training and testing loops which integrate with a simple and flexible callback system. This allows you to separate the basic logic of training from customisable side effects such as logging and checkpointing. We provide some useful pre-made callbacks and give examples for how to write your own.","title":"Stage 3: use the premade solstice.train() loop with solstice.Callbacks"},{"location":"#our-logos","text":"We have two Solstice logos: the Summer Solstice and the Winter Solstice . Both were created with Dall-E mini (free license) with the following prompt: a logo featuring stonehenge during a solstice","title":"Our Logos"},{"location":"contributing/","text":"Contributing \u00a4 This page looks nicer in the docs All contributions are welcome, including questions, issues, feature suggestions, and pull requests. There are two main parts to this codebase: abstract interfaces (such as solstice.Experiment and solstice.Metrics ) and concrete implementations (such as solstice.ClassificationExperiment and solstice.ClassificationMetrics ). Contributing concrete implementations is a good idea because it directly adds functionality for users without changing APIs. Changing the abstractions will require some discussion. My research focusses on computer vision, so this codebase will likely be biased towards things useful for that. Contributions helpful to other fields, such as NLP, are encouraged :) Info The supported method for installing the Solstice development environment is VSCode devcontainers . All you need is to have Docker and the VSCode 'Remote - Containers' extension installed. You can then install this project by running Remote containers: Clone Repository in Container Volume from the command palette (alternatively, you could clone the repository normally and run Remote Containers: Open folder in Container ). The default container is CPU only, but you can use GPU (CUDA 11.3) by following the instructions in devcontainer.json and rebuilding the container (you will need the NVIDIA container runtime installed). When making PRs, please ensure any changes are tested and documented. You can run tests by running pytest , and you can check the documentation locally by running mkdocs serve . Style Guide \u00a4 To match the style of the rest of the project, please follow these basic guidelines when making PRs. In general, if you follow the Google Python Style Guide , you should be fine. Code \u00a4 Tip We use Poetry for dependency management. You can add development dependencies with poetry add -D <package> . Black formatted (this will be automatic if using the devcontainer). Use type annotations for all parameters and return types. Try to avoid adding dependencies. If you need to use an import for type annotations, use the TYPE_CHECKING flag so that the dependency is only needed as a development one. Example Here, tensorflow is not imported at runtime, so it does not need to be a solstice dependency. It should be included as a development dependency for type checking. from typing import TYPE_CHECKING if TYPE_CHECKING : import tensorflow as tf def data_pipeline ( ds : tf . data . dataset ) -> tf . data . dataset : return ds . batch ( 32 ) . prefetch ( 1 ) Docs \u00a4 Tip Use boxes like this one to make docs look nicer. These are supported by the markdown Admonitions extension. We use mkdocstrings to allow documentation to be generated from docstrings. Favour documenting the API this way, as opposed to 'hard-coding' the documentation in the docs directory. Document all public API with Google-style docstrings (the devcontainer comes with the 'autodocstring' extension to make this easier). All methods in an abstract class (e.g. solstice.Experiment ) should be documented, including examples of usage and implementation tips. The methods for concrete implementation classes (e.g. solstice.ClassificationExperiment ) often don't need docstrings, as the abstract class already documents the interface. In this case, leave the docstring blank so the method will be excluded from the docs - this reduces clutter. Tests \u00a4","title":"Contributing"},{"location":"contributing/#contributing","text":"This page looks nicer in the docs All contributions are welcome, including questions, issues, feature suggestions, and pull requests. There are two main parts to this codebase: abstract interfaces (such as solstice.Experiment and solstice.Metrics ) and concrete implementations (such as solstice.ClassificationExperiment and solstice.ClassificationMetrics ). Contributing concrete implementations is a good idea because it directly adds functionality for users without changing APIs. Changing the abstractions will require some discussion. My research focusses on computer vision, so this codebase will likely be biased towards things useful for that. Contributions helpful to other fields, such as NLP, are encouraged :) Info The supported method for installing the Solstice development environment is VSCode devcontainers . All you need is to have Docker and the VSCode 'Remote - Containers' extension installed. You can then install this project by running Remote containers: Clone Repository in Container Volume from the command palette (alternatively, you could clone the repository normally and run Remote Containers: Open folder in Container ). The default container is CPU only, but you can use GPU (CUDA 11.3) by following the instructions in devcontainer.json and rebuilding the container (you will need the NVIDIA container runtime installed). When making PRs, please ensure any changes are tested and documented. You can run tests by running pytest , and you can check the documentation locally by running mkdocs serve .","title":"Contributing"},{"location":"contributing/#style-guide","text":"To match the style of the rest of the project, please follow these basic guidelines when making PRs. In general, if you follow the Google Python Style Guide , you should be fine.","title":"Style Guide"},{"location":"contributing/#code","text":"Tip We use Poetry for dependency management. You can add development dependencies with poetry add -D <package> . Black formatted (this will be automatic if using the devcontainer). Use type annotations for all parameters and return types. Try to avoid adding dependencies. If you need to use an import for type annotations, use the TYPE_CHECKING flag so that the dependency is only needed as a development one. Example Here, tensorflow is not imported at runtime, so it does not need to be a solstice dependency. It should be included as a development dependency for type checking. from typing import TYPE_CHECKING if TYPE_CHECKING : import tensorflow as tf def data_pipeline ( ds : tf . data . dataset ) -> tf . data . dataset : return ds . batch ( 32 ) . prefetch ( 1 )","title":"Code"},{"location":"contributing/#docs","text":"Tip Use boxes like this one to make docs look nicer. These are supported by the markdown Admonitions extension. We use mkdocstrings to allow documentation to be generated from docstrings. Favour documenting the API this way, as opposed to 'hard-coding' the documentation in the docs directory. Document all public API with Google-style docstrings (the devcontainer comes with the 'autodocstring' extension to make this easier). All methods in an abstract class (e.g. solstice.Experiment ) should be documented, including examples of usage and implementation tips. The methods for concrete implementation classes (e.g. solstice.ClassificationExperiment ) often don't need docstrings, as the abstract class already documents the interface. In this case, leave the docstring blank so the method will be excluded from the docs - this reduces clutter.","title":"Docs"},{"location":"contributing/#tests","text":"","title":"Tests"},{"location":"example_projects/","text":"Examples \u00a4 We provide full examples of Solstice usage in different settings. Each example is runnable as a standalone script. You can set up an environment for running the examples in just a few clicks by using the provided devcontainer; see the Contributing page for more information. mnist_from_scratch \u00a4 The MNIST example demonstrates how to implement everything yourself using just the Solstice base classes and Haiku to define the neural network. Summary This example implements: a solstice.Metrics class for keeping track of accuracy and loss a solstice.Experiment class for specifying how to train the Haiku model basic custom training and evaluation loops for running the experiment resnet_classification \u00a4 The ResNet example demonstrates how to perform image classification on CIFAR10 with data-parallel multi-gpu support. Uses Haiku to define the base neural net and Hydra for config management. Faq Run python examples/resnet_classification.py -h to see the available configuration options. Logging is done with TensorBoard. Run tensorboard --logdir=outputs to view the logs. Summary This example demonstrates: How to implement solstice.Experiment for training a ResNet50 with multi-gpu support. Usage of solstice.ClassificationMetrics for tracking metrics. Usage of solstice.LoggingCallback with TensorBoard (using the CLU SummaryWriter interface). Usage of solstice.ProfilingCallback for profiling with TensorBoard. Warning Multi-GPU support is not yet implemented. Currently only working with single GPU. adversarial_training \u00a4 The adversarial training example demonstrates how to implement custom training logic to remove bias from Colour MNIST (based on https://arxiv.org/abs/1812.10352 ). We use Equinox to define the base network. Summary This example demonstrates: How to implement solstice.Experiment for adversarially training a fair classifier. How to implement a custom solstice.Metrics class for tracking fairness-related metrics. How to implement a custom solstice.Callback for conditional early stopping. Usage of solstice.LoggingCallback with Weights and Biases integration. Warning Aspirational, not implemented yet. vmap_ensemble \u00a4 The vmap ensemble example demonstrates how to implement different implement parallelism strategies by training an ensemble of small neural networks simultaneously on one GPU (inspired by https://willwhitney.com/parallel-training-jax.html ). Uses Flax to define the base network. Summary This example implements: Warning Not Implemented Yet x_validation \u00a4 The X validation example demonstrates how to implement K folds cross validation with a custom training loop. Summary This example implements: Warning Not Implemented Yet","title":"Examples"},{"location":"example_projects/#examples","text":"We provide full examples of Solstice usage in different settings. Each example is runnable as a standalone script. You can set up an environment for running the examples in just a few clicks by using the provided devcontainer; see the Contributing page for more information.","title":"Examples"},{"location":"example_projects/#examples.mnist_from_scratch","text":"The MNIST example demonstrates how to implement everything yourself using just the Solstice base classes and Haiku to define the neural network. Summary This example implements: a solstice.Metrics class for keeping track of accuracy and loss a solstice.Experiment class for specifying how to train the Haiku model basic custom training and evaluation loops for running the experiment","title":"mnist_from_scratch"},{"location":"example_projects/#examples.resnet_classification","text":"The ResNet example demonstrates how to perform image classification on CIFAR10 with data-parallel multi-gpu support. Uses Haiku to define the base neural net and Hydra for config management. Faq Run python examples/resnet_classification.py -h to see the available configuration options. Logging is done with TensorBoard. Run tensorboard --logdir=outputs to view the logs. Summary This example demonstrates: How to implement solstice.Experiment for training a ResNet50 with multi-gpu support. Usage of solstice.ClassificationMetrics for tracking metrics. Usage of solstice.LoggingCallback with TensorBoard (using the CLU SummaryWriter interface). Usage of solstice.ProfilingCallback for profiling with TensorBoard. Warning Multi-GPU support is not yet implemented. Currently only working with single GPU.","title":"resnet_classification"},{"location":"example_projects/#examples.adversarial_training","text":"The adversarial training example demonstrates how to implement custom training logic to remove bias from Colour MNIST (based on https://arxiv.org/abs/1812.10352 ). We use Equinox to define the base network. Summary This example demonstrates: How to implement solstice.Experiment for adversarially training a fair classifier. How to implement a custom solstice.Metrics class for tracking fairness-related metrics. How to implement a custom solstice.Callback for conditional early stopping. Usage of solstice.LoggingCallback with Weights and Biases integration. Warning Aspirational, not implemented yet.","title":"adversarial_training"},{"location":"example_projects/#examples.vmap_ensemble","text":"The vmap ensemble example demonstrates how to implement different implement parallelism strategies by training an ensemble of small neural networks simultaneously on one GPU (inspired by https://willwhitney.com/parallel-training-jax.html ). Uses Flax to define the base network. Summary This example implements: Warning Not Implemented Yet","title":"vmap_ensemble"},{"location":"example_projects/#examples.x_validation","text":"The X validation example demonstrates how to implement K folds cross validation with a custom training loop. Summary This example implements: Warning Not Implemented Yet","title":"x_validation"},{"location":"from_flax_to_solstice/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); From Flax to Solstice \u00a4 This notebook starts with an MNIST classification project and demonstrates how to incrementally buy in to Solstice in 3 steps: Organise training code with solstice.Experiment Implement solstice.Metrics for tracking metrics Use the premade solstice.train() loop with solstice.Callback s Housekeeping: colab imports \u00a4 # if solstice isn't avaialble, we're in Colab, so import extra packages # else we assume you've set up the devcontainer so install no extras try : import solstice except ImportError : ... % pip install solstice - jax % pip install flax % pip install optax MNIST in pure Flax \u00a4 First, set up the dataset: % env XLA_PYTHON_CLIENT_PREALLOCATE = false import tensorflow as tf import tensorflow_datasets as tfds # stop tensorflow grabbing GPU memory tf . config . experimental . set_visible_devices ([], \"GPU\" ) train_ds = tfds . load ( name = \"mnist\" , split = \"train\" , as_supervised = True , data_dir = \"/tmp/data\" ) assert isinstance ( train_ds , tf . data . Dataset ) preprocess_mnist = lambda x , y : ( tf . reshape ( tf . cast ( x , tf . float32 ) / 255 , ( 784 ,)), tf . cast ( y , tf . float32 ), ) train_ds = train_ds . map ( preprocess_mnist ) . batch ( 32 ) . prefetch ( 1 ) env: XLA_PYTHON_CLIENT_PREALLOCATE=false /opt/venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm 2022-07-09 12:07:03.558482: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\". Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /tmp/data/mnist/3.0.1... Dl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.57 file/s] Dataset mnist downloaded and prepared to /tmp/data/mnist/3.0.1. Subsequent calls will reuse this data. Now, create the Flax model: from typing import Sequence , Any import flax.linen as nn import jax.numpy as jnp class MLP ( nn . Module ): features : Sequence [ int ] dtype : Any = jnp . float32 @nn . compact def __call__ ( self , x ): for i , feat in enumerate ( self . features ): x = nn . Dense ( feat , dtype = self . dtype )( x ) if i != len ( self . features ) - 1 : x = nn . relu ( x ) return x Now, define a TrainState object and training step (notice how this is already quite similar to solstice.Experiment ): from typing import Callable , Tuple import jax import optax import dataclasses from flax import struct @struct . dataclass class TrainState : params : optax . Params opt_state : optax . OptState tx : optax . GradientTransformation = struct . field ( pytree_node = False ) apply_fn : Callable = struct . field ( pytree_node = False ) @classmethod def create ( cls , rng : int , learning_rate : float ): key = jax . random . PRNGKey ( rng ) model = MLP ( features = [ 300 , 300 , 10 ]) params = model . init ( key , jnp . ones ([ 1 , 784 ]))[ 'params' ] tx = optax . sgd ( learning_rate ) opt_state = tx . init ( params ) return cls ( params , opt_state , tx , model . apply ) @jax . jit def train_step ( state : TrainState , batch : Tuple [ jnp . ndarray , jnp . ndarray ] ) -> Tuple [ TrainState , Any ]: imgs , labels = batch def loss_fn ( params ): logits = state . apply_fn ({ 'params' : params }, imgs ) loss = jnp . mean ( optax . softmax_cross_entropy ( logits , jax . nn . one_hot ( labels , 10 ))) return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( state . params ) updates , new_opt_state = state . tx . update ( grads , state . opt_state , state . params ) new_params = optax . apply_updates ( state . params , updates ) preds = jnp . argmax ( logits , axis =- 1 ) accuracy = jnp . mean ( preds == labels ) metrics = { 'accuracy' : accuracy , 'loss' : loss } return dataclasses . replace ( state , params = new_params , opt_state = new_opt_state ), metrics Finally, make a training loop and train the model: from tqdm import tqdm def flax_train ( state : TrainState , train_ds : tf . data . Dataset , num_epochs : int ): metrics = [] for epoch in range ( num_epochs ): for batch in tqdm ( train_ds . as_numpy_iterator (), total = len ( train_ds )): state , batch_metrics = train_step ( state , batch ) metrics . append ( batch_metrics ) metrics = jax . tree_util . tree_map ( lambda * ms : jnp . mean ( jnp . array ( ms )), * metrics ) print ( f \"Epoch { epoch } , { metrics } \" ) metrics = [] return state state = TrainState . create ( rng = 0 , learning_rate = 0.1 ) trained_state = flax_train ( state , train_ds , num_epochs = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:03<00:00, 478.66it/s] Epoch 0, {'accuracy': DeviceArray(0.9185667, dtype=float32), 'loss': DeviceArray(0.27219555, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1332.18it/s] Epoch 1, {'accuracy': DeviceArray(0.96816665, dtype=float32), 'loss': DeviceArray(0.10662512, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1247.04it/s] Epoch 2, {'accuracy': DeviceArray(0.97900003, dtype=float32), 'loss': DeviceArray(0.07093416, dtype=float32)} Introducing solstice.Experiment \u00a4 Here, we introduce the solstice.Experiment , a better way to organise your deep learning code. When converting from Flax to Solstice, notice that a couple of things happened: We replaced TrainState with solstice.Experiment , using __init__ instead of .create We encapsulated the train_step() function into a train_step() method. All mentions of state became mentions of self . You can (optionally) use filtered transformations instead of specifying fields as static up-front (see the Solstice Primer for more info). Notice that self is just a PyTree, and the train_step method is still a pure function. Like TrainState , Experiment s are immutable, so all updates are performed out-of-place by returning a new Experiment from the step. import equinox as eqx import solstice class MNISTClassifier ( solstice . Experiment ): params : optax . Params opt_state : optax . OptState tx : optax . GradientTransformation = eqx . static_field () apply_fn : Callable = eqx . static_field () def __init__ ( self , rng : int , learning_rate : float ): key = jax . random . PRNGKey ( rng ) model = MLP ( features = [ 300 , 300 , 10 ]) self . params = model . init ( key , jnp . ones ([ 1 , 784 ]))[ 'params' ] self . tx = optax . sgd ( learning_rate ) self . opt_state = self . tx . init ( self . params ) self . apply_fn = model . apply @jax . jit def train_step ( self , batch : Tuple [ jnp . ndarray , jnp . ndarray ]) -> Tuple [ \"MNISTClassifier\" , Any ]: imgs , labels = batch def loss_fn ( params ): logits = self . apply_fn ({ 'params' : params }, imgs ) loss = jnp . mean ( optax . softmax_cross_entropy ( logits , jax . nn . one_hot ( labels , 10 ))) return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( self . params ) updates , new_opt_state = self . tx . update ( grads , self . opt_state , self . params ) new_params = optax . apply_updates ( self . params , updates ) preds = jnp . argmax ( logits , axis =- 1 ) accuracy = jnp . mean ( preds == labels ) metrics = { 'accuracy' : accuracy , 'loss' : loss } return solstice . replace ( self , params = new_params , opt_state = new_opt_state ), metrics def eval_step ( self , batch ): raise NotImplementedError ( \"not bothering with eval in this example\" ) def solstice_train ( exp : solstice . Experiment , train_ds : tf . data . Dataset , num_epochs : int ): metrics = [] for epoch in range ( num_epochs ): for batch in tqdm ( train_ds . as_numpy_iterator (), total = len ( train_ds )): exp , batch_metrics = exp . train_step ( batch ) metrics . append ( batch_metrics ) metrics = jax . tree_util . tree_map ( lambda * ms : jnp . mean ( jnp . array ( ms )), * metrics ) print ( f \"Epoch { epoch } , { metrics } \" ) metrics = [] return exp exp = MNISTClassifier ( rng = 0 , learning_rate = 0.1 ) trained_exp = solstice_train ( exp , train_ds , num_epochs = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1108.96it/s] Epoch 0, {'accuracy': DeviceArray(0.9185667, dtype=float32), 'loss': DeviceArray(0.27219555, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1098.72it/s] Epoch 1, {'accuracy': DeviceArray(0.96816665, dtype=float32), 'loss': DeviceArray(0.10662512, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1216.60it/s] Epoch 2, {'accuracy': DeviceArray(0.97900003, dtype=float32), 'loss': DeviceArray(0.07093416, dtype=float32)} Notice that none of the logic has changed (in fact all the computations and results are identical), just the organisation. Even without the rest of Solstice, this has a few advantages over the pure Flax code: Better ergonomics due to creating experiments with __init__ instead of custom classmethods. Explicitly keeping related training code together in one place. The flax code had implicit coupling between the train_step() and TrainState , it is now encapsulated into one class to make the dependency explicit. It is now easier to define different Experiment classes for different experiments and sweep across them with with your favourite tools (such as hydra or wandb). Introducing solstice.Metrics \u00a4 Did you notice the subtle gotcha in the metrics calculation above? The dataset size needed to be perfectly divisible by the batch size, otherwise the last batch would have had a different size so averaging the loss and accuracy over all batches would have been wrong. Accumulating and calculating metrics gets even harder when you are using metrics that are not 'averageable' such as precision. We provide solstice.Metrics , an API for keeping track of metrics scalably and without these headaches. A solstice.Metrics object knows how to do three things: - Calculate intermediate results from model outputs with __init__ . - Accumulate results with other solstice.Metrics objects with merge() . - Calculate final metrics with compute() . Below, we integrate this into our current MNIST experiment, notice that the results are still the same, but the code is cleaner and more extensible: from typing import Mapping class MyMetrics ( solstice . Metrics ): \"\"\"Custom Metrics class for calculating accuracy and average loss. Included for didactic purposes, in practice `solstice.ClassificationMetrics` is better.\"\"\" average_loss : float count : int # number of samples seen num_correct : int def __init__ ( self , preds : jnp . ndarray , targets : jnp . ndarray , loss : float ) -> None : self . average_loss = loss self . count = preds . shape [ 0 ] # assumes batch is first dim self . num_correct = jnp . sum ( preds == targets ) def merge ( self , other : \"MyMetrics\" ) -> \"MyMetrics\" : # can simply sum num_correct and count new_num_correct = self . num_correct + other . num_correct new_count = self . count + other . count # average loss is weighted by count from each object new_loss = ( self . average_loss * self . count + other . average_loss * other . count ) / ( self . count + other . count ) return solstice . replace ( self , num_correct = new_num_correct , count = new_count , average_loss = new_loss ) def compute ( self ) -> Mapping [ str , float ]: return { \"accuracy\" : self . num_correct / self . count , \"average_loss\" : self . average_loss , } class MNISTClassifierWithMetrics ( solstice . Experiment ): params : optax . Params opt_state : optax . OptState tx : optax . GradientTransformation = eqx . static_field () apply_fn : Callable = eqx . static_field () def __init__ ( self , rng : int , learning_rate : float ): key = jax . random . PRNGKey ( rng ) model = MLP ( features = [ 300 , 300 , 10 ]) self . params = model . init ( key , jnp . ones ([ 1 , 784 ]))[ 'params' ] self . tx = optax . sgd ( learning_rate ) self . opt_state = self . tx . init ( self . params ) self . apply_fn = model . apply @jax . jit def train_step ( self , batch : Tuple [ jnp . ndarray , jnp . ndarray ]) -> Tuple [ \"MNISTClassifierWithMetrics\" , solstice . Metrics ]: imgs , labels = batch def loss_fn ( params ): logits = self . apply_fn ({ 'params' : params }, imgs ) loss = jnp . mean ( optax . softmax_cross_entropy ( logits , jax . nn . one_hot ( labels , 10 ))) return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( self . params ) updates , new_opt_state = self . tx . update ( grads , self . opt_state , self . params ) new_params = optax . apply_updates ( self . params , updates ) preds = jnp . argmax ( logits , axis =- 1 ) metrics = MyMetrics ( preds , labels , loss ) return solstice . replace ( self , params = new_params , opt_state = new_opt_state ), metrics def eval_step ( self , batch ): raise NotImplementedError ( \"not bothering with eval in this example\" ) def solstice_train_with_metrics ( exp : solstice . Experiment , train_ds : tf . data . Dataset , num_epochs : int ): metrics = None for epoch in range ( num_epochs ): for batch in tqdm ( train_ds . as_numpy_iterator (), total = len ( train_ds )): exp , batch_metrics = exp . train_step ( batch ) metrics = batch_metrics if metrics is None else batch_metrics . merge ( metrics ) assert metrics is not None metrics = metrics . compute () print ( f \"Epoch { epoch } , { metrics } \" ) metrics = None return exp exp = MNISTClassifierWithMetrics ( rng = 0 , learning_rate = 0.1 ) trained_exp = solstice_train_with_metrics ( exp , train_ds , num_epochs = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:02<00:00, 629.13it/s] Epoch 0, {'accuracy': DeviceArray(0.9185667, dtype=float32), 'average_loss': DeviceArray(0.27219722, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:02<00:00, 848.14it/s] Epoch 1, {'accuracy': DeviceArray(0.96816665, dtype=float32), 'average_loss': DeviceArray(0.10662578, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:02<00:00, 770.72it/s] Epoch 2, {'accuracy': DeviceArray(0.97900003, dtype=float32), 'average_loss': DeviceArray(0.07093462, dtype=float32)} Solstice also provides some pre-made metrics classes, such as solstice.ClassificationMetrics for common use cases. Introducing solstice.train() and solstice.Callback s \u00a4 Often, training loops are boilerplate code. In general, they tend to have two parts: the loops that advance the training state, and the bits that make side effects such as logging and checkpointing work. Solstice comes with solstice.train() , a standard training loop which integrates with a flexible callback system for injecting side effects. Below, we use the built-in solstice.LoggingCallback with solstice.train() to cut down on boilerplate code. import logging logging . getLogger ( \"solstice\" ) . setLevel ( logging . INFO ) # by default, `solstice.LoggingCallback` logs to the built-in Python logging system # with name 'solstice' and level INFO. You can also use this callback with TensorBoard etc... logging_callback = solstice . LoggingCallback () exp = MNISTClassifierWithMetrics ( rng = 0 , learning_rate = 0.1 ) trained_exp = solstice . train ( exp , train_ds = train_ds , num_epochs = 3 , callbacks = [ logging_callback ]) Training: 0%| | 0/3 [00:00<?, ?epoch/s]INFO:solstice:train step 0: {'accuracy': DeviceArray(0.9185667, dtype=float32), 'average_loss': DeviceArray(0.27219722, dtype=float32)} Training: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:02<00:05, 2.61s/epoch]INFO:solstice:train step 1: {'accuracy': DeviceArray(0.96816665, dtype=float32), 'average_loss': DeviceArray(0.10662578, dtype=float32)} Training: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:05<00:02, 2.60s/epoch]INFO:solstice:train step 2: {'accuracy': DeviceArray(0.97900003, dtype=float32), 'average_loss': DeviceArray(0.07093462, dtype=float32)} Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:07<00:00, 2.51s/epoch] Notice that the results are still identical to the ones from the initial Flax code. All Solstice does is provide user-facing utilities for creating and scaling deep learning experiments in JAX. We encourage people to create their own Callback s to do more interesting things.","title":"From flax to solstice"},{"location":"from_flax_to_solstice/#from-flax-to-solstice","text":"This notebook starts with an MNIST classification project and demonstrates how to incrementally buy in to Solstice in 3 steps: Organise training code with solstice.Experiment Implement solstice.Metrics for tracking metrics Use the premade solstice.train() loop with solstice.Callback s","title":"From Flax to Solstice"},{"location":"from_flax_to_solstice/#housekeeping-colab-imports","text":"# if solstice isn't avaialble, we're in Colab, so import extra packages # else we assume you've set up the devcontainer so install no extras try : import solstice except ImportError : ... % pip install solstice - jax % pip install flax % pip install optax","title":"Housekeeping: colab imports"},{"location":"from_flax_to_solstice/#mnist-in-pure-flax","text":"First, set up the dataset: % env XLA_PYTHON_CLIENT_PREALLOCATE = false import tensorflow as tf import tensorflow_datasets as tfds # stop tensorflow grabbing GPU memory tf . config . experimental . set_visible_devices ([], \"GPU\" ) train_ds = tfds . load ( name = \"mnist\" , split = \"train\" , as_supervised = True , data_dir = \"/tmp/data\" ) assert isinstance ( train_ds , tf . data . Dataset ) preprocess_mnist = lambda x , y : ( tf . reshape ( tf . cast ( x , tf . float32 ) / 255 , ( 784 ,)), tf . cast ( y , tf . float32 ), ) train_ds = train_ds . map ( preprocess_mnist ) . batch ( 32 ) . prefetch ( 1 ) env: XLA_PYTHON_CLIENT_PREALLOCATE=false /opt/venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm 2022-07-09 12:07:03.558482: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\". Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /tmp/data/mnist/3.0.1... Dl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00, 2.57 file/s] Dataset mnist downloaded and prepared to /tmp/data/mnist/3.0.1. Subsequent calls will reuse this data. Now, create the Flax model: from typing import Sequence , Any import flax.linen as nn import jax.numpy as jnp class MLP ( nn . Module ): features : Sequence [ int ] dtype : Any = jnp . float32 @nn . compact def __call__ ( self , x ): for i , feat in enumerate ( self . features ): x = nn . Dense ( feat , dtype = self . dtype )( x ) if i != len ( self . features ) - 1 : x = nn . relu ( x ) return x Now, define a TrainState object and training step (notice how this is already quite similar to solstice.Experiment ): from typing import Callable , Tuple import jax import optax import dataclasses from flax import struct @struct . dataclass class TrainState : params : optax . Params opt_state : optax . OptState tx : optax . GradientTransformation = struct . field ( pytree_node = False ) apply_fn : Callable = struct . field ( pytree_node = False ) @classmethod def create ( cls , rng : int , learning_rate : float ): key = jax . random . PRNGKey ( rng ) model = MLP ( features = [ 300 , 300 , 10 ]) params = model . init ( key , jnp . ones ([ 1 , 784 ]))[ 'params' ] tx = optax . sgd ( learning_rate ) opt_state = tx . init ( params ) return cls ( params , opt_state , tx , model . apply ) @jax . jit def train_step ( state : TrainState , batch : Tuple [ jnp . ndarray , jnp . ndarray ] ) -> Tuple [ TrainState , Any ]: imgs , labels = batch def loss_fn ( params ): logits = state . apply_fn ({ 'params' : params }, imgs ) loss = jnp . mean ( optax . softmax_cross_entropy ( logits , jax . nn . one_hot ( labels , 10 ))) return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( state . params ) updates , new_opt_state = state . tx . update ( grads , state . opt_state , state . params ) new_params = optax . apply_updates ( state . params , updates ) preds = jnp . argmax ( logits , axis =- 1 ) accuracy = jnp . mean ( preds == labels ) metrics = { 'accuracy' : accuracy , 'loss' : loss } return dataclasses . replace ( state , params = new_params , opt_state = new_opt_state ), metrics Finally, make a training loop and train the model: from tqdm import tqdm def flax_train ( state : TrainState , train_ds : tf . data . Dataset , num_epochs : int ): metrics = [] for epoch in range ( num_epochs ): for batch in tqdm ( train_ds . as_numpy_iterator (), total = len ( train_ds )): state , batch_metrics = train_step ( state , batch ) metrics . append ( batch_metrics ) metrics = jax . tree_util . tree_map ( lambda * ms : jnp . mean ( jnp . array ( ms )), * metrics ) print ( f \"Epoch { epoch } , { metrics } \" ) metrics = [] return state state = TrainState . create ( rng = 0 , learning_rate = 0.1 ) trained_state = flax_train ( state , train_ds , num_epochs = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:03<00:00, 478.66it/s] Epoch 0, {'accuracy': DeviceArray(0.9185667, dtype=float32), 'loss': DeviceArray(0.27219555, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1332.18it/s] Epoch 1, {'accuracy': DeviceArray(0.96816665, dtype=float32), 'loss': DeviceArray(0.10662512, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1247.04it/s] Epoch 2, {'accuracy': DeviceArray(0.97900003, dtype=float32), 'loss': DeviceArray(0.07093416, dtype=float32)}","title":"MNIST in pure Flax"},{"location":"from_flax_to_solstice/#introducing-solsticeexperiment","text":"Here, we introduce the solstice.Experiment , a better way to organise your deep learning code. When converting from Flax to Solstice, notice that a couple of things happened: We replaced TrainState with solstice.Experiment , using __init__ instead of .create We encapsulated the train_step() function into a train_step() method. All mentions of state became mentions of self . You can (optionally) use filtered transformations instead of specifying fields as static up-front (see the Solstice Primer for more info). Notice that self is just a PyTree, and the train_step method is still a pure function. Like TrainState , Experiment s are immutable, so all updates are performed out-of-place by returning a new Experiment from the step. import equinox as eqx import solstice class MNISTClassifier ( solstice . Experiment ): params : optax . Params opt_state : optax . OptState tx : optax . GradientTransformation = eqx . static_field () apply_fn : Callable = eqx . static_field () def __init__ ( self , rng : int , learning_rate : float ): key = jax . random . PRNGKey ( rng ) model = MLP ( features = [ 300 , 300 , 10 ]) self . params = model . init ( key , jnp . ones ([ 1 , 784 ]))[ 'params' ] self . tx = optax . sgd ( learning_rate ) self . opt_state = self . tx . init ( self . params ) self . apply_fn = model . apply @jax . jit def train_step ( self , batch : Tuple [ jnp . ndarray , jnp . ndarray ]) -> Tuple [ \"MNISTClassifier\" , Any ]: imgs , labels = batch def loss_fn ( params ): logits = self . apply_fn ({ 'params' : params }, imgs ) loss = jnp . mean ( optax . softmax_cross_entropy ( logits , jax . nn . one_hot ( labels , 10 ))) return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( self . params ) updates , new_opt_state = self . tx . update ( grads , self . opt_state , self . params ) new_params = optax . apply_updates ( self . params , updates ) preds = jnp . argmax ( logits , axis =- 1 ) accuracy = jnp . mean ( preds == labels ) metrics = { 'accuracy' : accuracy , 'loss' : loss } return solstice . replace ( self , params = new_params , opt_state = new_opt_state ), metrics def eval_step ( self , batch ): raise NotImplementedError ( \"not bothering with eval in this example\" ) def solstice_train ( exp : solstice . Experiment , train_ds : tf . data . Dataset , num_epochs : int ): metrics = [] for epoch in range ( num_epochs ): for batch in tqdm ( train_ds . as_numpy_iterator (), total = len ( train_ds )): exp , batch_metrics = exp . train_step ( batch ) metrics . append ( batch_metrics ) metrics = jax . tree_util . tree_map ( lambda * ms : jnp . mean ( jnp . array ( ms )), * metrics ) print ( f \"Epoch { epoch } , { metrics } \" ) metrics = [] return exp exp = MNISTClassifier ( rng = 0 , learning_rate = 0.1 ) trained_exp = solstice_train ( exp , train_ds , num_epochs = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1108.96it/s] Epoch 0, {'accuracy': DeviceArray(0.9185667, dtype=float32), 'loss': DeviceArray(0.27219555, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1098.72it/s] Epoch 1, {'accuracy': DeviceArray(0.96816665, dtype=float32), 'loss': DeviceArray(0.10662512, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:01<00:00, 1216.60it/s] Epoch 2, {'accuracy': DeviceArray(0.97900003, dtype=float32), 'loss': DeviceArray(0.07093416, dtype=float32)} Notice that none of the logic has changed (in fact all the computations and results are identical), just the organisation. Even without the rest of Solstice, this has a few advantages over the pure Flax code: Better ergonomics due to creating experiments with __init__ instead of custom classmethods. Explicitly keeping related training code together in one place. The flax code had implicit coupling between the train_step() and TrainState , it is now encapsulated into one class to make the dependency explicit. It is now easier to define different Experiment classes for different experiments and sweep across them with with your favourite tools (such as hydra or wandb).","title":"Introducing solstice.Experiment"},{"location":"from_flax_to_solstice/#introducing-solsticemetrics","text":"Did you notice the subtle gotcha in the metrics calculation above? The dataset size needed to be perfectly divisible by the batch size, otherwise the last batch would have had a different size so averaging the loss and accuracy over all batches would have been wrong. Accumulating and calculating metrics gets even harder when you are using metrics that are not 'averageable' such as precision. We provide solstice.Metrics , an API for keeping track of metrics scalably and without these headaches. A solstice.Metrics object knows how to do three things: - Calculate intermediate results from model outputs with __init__ . - Accumulate results with other solstice.Metrics objects with merge() . - Calculate final metrics with compute() . Below, we integrate this into our current MNIST experiment, notice that the results are still the same, but the code is cleaner and more extensible: from typing import Mapping class MyMetrics ( solstice . Metrics ): \"\"\"Custom Metrics class for calculating accuracy and average loss. Included for didactic purposes, in practice `solstice.ClassificationMetrics` is better.\"\"\" average_loss : float count : int # number of samples seen num_correct : int def __init__ ( self , preds : jnp . ndarray , targets : jnp . ndarray , loss : float ) -> None : self . average_loss = loss self . count = preds . shape [ 0 ] # assumes batch is first dim self . num_correct = jnp . sum ( preds == targets ) def merge ( self , other : \"MyMetrics\" ) -> \"MyMetrics\" : # can simply sum num_correct and count new_num_correct = self . num_correct + other . num_correct new_count = self . count + other . count # average loss is weighted by count from each object new_loss = ( self . average_loss * self . count + other . average_loss * other . count ) / ( self . count + other . count ) return solstice . replace ( self , num_correct = new_num_correct , count = new_count , average_loss = new_loss ) def compute ( self ) -> Mapping [ str , float ]: return { \"accuracy\" : self . num_correct / self . count , \"average_loss\" : self . average_loss , } class MNISTClassifierWithMetrics ( solstice . Experiment ): params : optax . Params opt_state : optax . OptState tx : optax . GradientTransformation = eqx . static_field () apply_fn : Callable = eqx . static_field () def __init__ ( self , rng : int , learning_rate : float ): key = jax . random . PRNGKey ( rng ) model = MLP ( features = [ 300 , 300 , 10 ]) self . params = model . init ( key , jnp . ones ([ 1 , 784 ]))[ 'params' ] self . tx = optax . sgd ( learning_rate ) self . opt_state = self . tx . init ( self . params ) self . apply_fn = model . apply @jax . jit def train_step ( self , batch : Tuple [ jnp . ndarray , jnp . ndarray ]) -> Tuple [ \"MNISTClassifierWithMetrics\" , solstice . Metrics ]: imgs , labels = batch def loss_fn ( params ): logits = self . apply_fn ({ 'params' : params }, imgs ) loss = jnp . mean ( optax . softmax_cross_entropy ( logits , jax . nn . one_hot ( labels , 10 ))) return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( self . params ) updates , new_opt_state = self . tx . update ( grads , self . opt_state , self . params ) new_params = optax . apply_updates ( self . params , updates ) preds = jnp . argmax ( logits , axis =- 1 ) metrics = MyMetrics ( preds , labels , loss ) return solstice . replace ( self , params = new_params , opt_state = new_opt_state ), metrics def eval_step ( self , batch ): raise NotImplementedError ( \"not bothering with eval in this example\" ) def solstice_train_with_metrics ( exp : solstice . Experiment , train_ds : tf . data . Dataset , num_epochs : int ): metrics = None for epoch in range ( num_epochs ): for batch in tqdm ( train_ds . as_numpy_iterator (), total = len ( train_ds )): exp , batch_metrics = exp . train_step ( batch ) metrics = batch_metrics if metrics is None else batch_metrics . merge ( metrics ) assert metrics is not None metrics = metrics . compute () print ( f \"Epoch { epoch } , { metrics } \" ) metrics = None return exp exp = MNISTClassifierWithMetrics ( rng = 0 , learning_rate = 0.1 ) trained_exp = solstice_train_with_metrics ( exp , train_ds , num_epochs = 3 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:02<00:00, 629.13it/s] Epoch 0, {'accuracy': DeviceArray(0.9185667, dtype=float32), 'average_loss': DeviceArray(0.27219722, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:02<00:00, 848.14it/s] Epoch 1, {'accuracy': DeviceArray(0.96816665, dtype=float32), 'average_loss': DeviceArray(0.10662578, dtype=float32)} 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1875/1875 [00:02<00:00, 770.72it/s] Epoch 2, {'accuracy': DeviceArray(0.97900003, dtype=float32), 'average_loss': DeviceArray(0.07093462, dtype=float32)} Solstice also provides some pre-made metrics classes, such as solstice.ClassificationMetrics for common use cases.","title":"Introducing solstice.Metrics"},{"location":"from_flax_to_solstice/#introducing-solsticetrain-and-solsticecallbacks","text":"Often, training loops are boilerplate code. In general, they tend to have two parts: the loops that advance the training state, and the bits that make side effects such as logging and checkpointing work. Solstice comes with solstice.train() , a standard training loop which integrates with a flexible callback system for injecting side effects. Below, we use the built-in solstice.LoggingCallback with solstice.train() to cut down on boilerplate code. import logging logging . getLogger ( \"solstice\" ) . setLevel ( logging . INFO ) # by default, `solstice.LoggingCallback` logs to the built-in Python logging system # with name 'solstice' and level INFO. You can also use this callback with TensorBoard etc... logging_callback = solstice . LoggingCallback () exp = MNISTClassifierWithMetrics ( rng = 0 , learning_rate = 0.1 ) trained_exp = solstice . train ( exp , train_ds = train_ds , num_epochs = 3 , callbacks = [ logging_callback ]) Training: 0%| | 0/3 [00:00<?, ?epoch/s]INFO:solstice:train step 0: {'accuracy': DeviceArray(0.9185667, dtype=float32), 'average_loss': DeviceArray(0.27219722, dtype=float32)} Training: 33%|\u2588\u2588\u2588\u258e | 1/3 [00:02<00:05, 2.61s/epoch]INFO:solstice:train step 1: {'accuracy': DeviceArray(0.96816665, dtype=float32), 'average_loss': DeviceArray(0.10662578, dtype=float32)} Training: 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 2/3 [00:05<00:02, 2.60s/epoch]INFO:solstice:train step 2: {'accuracy': DeviceArray(0.97900003, dtype=float32), 'average_loss': DeviceArray(0.07093462, dtype=float32)} Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:07<00:00, 2.51s/epoch] Notice that the results are still identical to the ones from the initial Flax code. All Solstice does is provide user-facing utilities for creating and scaling deep learning experiments in JAX. We encourage people to create their own Callback s to do more interesting things.","title":"Introducing solstice.train() and solstice.Callbacks"},{"location":"parallelism_strategies/","text":"Parallelism Strategies \u00a4","title":"Parallelism Strategies"},{"location":"parallelism_strategies/#parallelism-strategies","text":"","title":"Parallelism Strategies"},{"location":"primer/","text":"Primer - Background and Concepts \u00a4 Here, we write about the concepts behind Solstice. It is not necessary to understand all of it before getting started, but it might help to understand our design. The Problem: coupling in ML research code \u00a4 In machine learning projects, you generally have at least four main parts of your code: model dataset training strategy (i.e. optimization, train/eval step logic, metrics calculation) training/testing loops (including logging, checkpointing etc...) In many research projects, it is helpful to be able to swap out any of these parts on a whim (ideally, just by changing your config files). In practice, it follows that the better you can decouple these four parts of your code, the faster you can iterate your experiments. Much attention is paid to the four components individually, but many researchers then just throw everything together haphazardly. It can require quite a significant engineering effort to properly decouple all the components, so most people don't bother. Wouldn't it be great if there was a standard way of organising your code to rapidly scale and iterate experiments... Related Work / Why Solstice? \u00a4 We are not the first people to notice the usefulness of a well-engineered experiment library for deep learning. PyTorch Lightning has filled this niche for PyTorch and Sonnet/Keras have done the same for TensorFlow. In JAX, however, there is currently a cambrian explosion of libraries, e.g. Jaxline, Scenic, Objax, Elegy, CLU. All of these works are excellent, but they each have pros and cons. Libraries such as Jaxline, Objax, and Scenic are more 'framework-ish' - they ask you to do things their way and in return they do a lot for you. The natural tradeoff with this style of library is that you are locked in to their entire ecosystem which can result in a lack of flexibility. Of these, Elegy is the most closely related to Solstice due to its everything-is-a-pytree approach, however, where Elegy aims to be Keras-like, we aim to be lighter-weight. In Solstice, we leave nearly everything to the user and have incremental buy-in as a top design priority. CLU (for Flax) is also closely related, with Solstice having a similar Metrics API, as well as a similar philosophy of providing flexible utilities for common use cases. Surprisingly, Flax itself comes quite close to Solstice with its flax.training.TrainState concept. This is an object which holds experiment state with related functions and, taken to the extreme, can end up looking very similar to a solstice.Experiment . Equinox is the foundation of this project and is an elegant library for 'callable pytrees' in JAX (we explain this further below). Where Equinox strives to be as general as possible, we offer opinions about how to structure deep learning experiments. Solstice is a small library, implemented entirely in pure JAX+Equinox. Like Equinox, the simplicity of Solstice is its selling point. The key idea: immutable classes in Python \u00a4 In object-oriented code, classes encapsulate both state and transformations on that state. Crucially, the state is mutable, so calling a method might return nothing but alter the object's state. This is a side effect . In functional programming, side effects are avoided by making state immutable, so instead of calling a method to update an object's state, you could pass the object to a pure function, returning a new object with the altered state. This is generally an easier paradigm to reason about and immutability is also needed in JAX for XLA to work its magic. This is all great, but Python is not really designed for the functional paradigm so it is difficult to fully decouple all the parts of your code. Type hinting functions with Protocols can get you surprisingly far, but at some point you will probably want to achieve some level of encapsulation and use abstract base classes to get dependency inversion. The approach we take in Solstice is to use immutable dataclasses to try to get the best of both worlds, the code below shows how you would implement a simple counter in each of the paradigms. OO-Style Functional-Style Solstice/Equinox-Style class OOPCounter : def __init__ ( self , initial_value : int = 0 ) -> None : self . count = initial_value def increment ( self ) -> None : self . count += 1 # 'initialise' the OO counter counter = OOPCounter () assert counter . count == 0 start_id = id ( counter ) # 'apply' the increment method counter . increment () assert counter . count == 1 end_id = id ( counter ) assert start_id == end_id def increment_fn ( current_value : int ) -> int : return current_value + 1 # 'initialise' the functional counter count = 0 assert count == 0 start_id = id ( count ) # 'apply' the increment func count = increment_fn ( count ) assert count == 1 end_id = id ( count ) assert start_id != end_id import dataclasses @dataclasses . dataclass ( frozen = True ) class SolsticeStyleCounter : count : int = 0 def increment ( self ) -> \"SolsticeStyleCounter\" : return dataclasses . replace ( self , count = self . count + 1 ) # 'initialise' the SolsticeStyleCounter counter = SolsticeStyleCounter () assert counter . count == 0 start_id = id ( counter ) # 'apply' the increment method, returning a new state object counter = counter . increment () assert counter . count == 1 end_id = id ( counter ) assert start_id != end_id Notice that the Solstice style counter did not mutate its state, it returned a new instance of itself with the new state. The great thing about this pattern is that by keeping our data structures immutable, we get to keep the readability and XLA optimization advantages that come with it and we also get all the power of Python classes and OO-ish design patterns. Warning The examples above use dataclasses.replace to perform out-of-place updates. This is a limited solution because it breaks when custom __init__() constructors are defined. Fortunately, Equinox has solved this problem with equinox.tree_at ; Solstice provides solstice.replace , a wrapped version of this function matching the dataclasses.replace syntax. See https://github.com/patrick-kidger/equinox/issues/120 for more details. In practice, in machine learning, this means we can replace the common init/apply pure functions with methods in a frozen dataclass (usually __init__() , and __call__() ). There is also one final matter to take care of... JAX only operates on PyTrees and doesn't know how to deal with dataclasses. This is why we build Solstice on top of Equinox, because an equinox.Module is just a dataclass which is registered as a PyTree. This is a powerful paradigm, and it allows us to trivially do things which are considerably more difficult in Flax/Haiku, like specifying common interfaces for models using abstract base classes: from abc import ABC , abstractmethod import equinox as eqx import jax.numpy as jnp class EncoderDecoderModel ( eqx . Module , ABC ): \"\"\"Encoder-Decoder models (e.g. VAEs) implement this interface.\"\"\" @abstractmethod def __init__ ( self ): \"\"\"Initialise model parameters.\"\"\" pass @abstractmethod def encode ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Encode the input data into a latent representation.\"\"\" pass @abstractmethod def decode ( self , z : jnp . ndarray ) -> jnp . ndarray : \"\"\"Decode the latent representation.\"\"\" pass All classes in Solstice (except the training loop ones) inherit from equinox.Module . To get started, your mental model can simply be that an equinox.Module is a frozen dataclass that JAX can interpret, although it helps to understand the ideas behind Equinox a bit. Equinox and filtered transformations \u00a4 Filtered transformations are explained fully in the Equinox Docs , but we include a quick primer here for completeness. When we use Equinox to create class-style PyTrees, we are able to attach things to the PyTree that JAX function transformations don't know how to interpret. For example, you might store a callable, like your model's apply() function in a solstice.Experiment . Now, if you try to jax.jit a method in the experiment which takes self as a parameter, JAX with throw an error because it doesn't know how to JIT with callables as inputs to the function (usually, you would have to specify the argument as static). Equinox gets around this problem with filtered transformations, which involve splitting the PyTree into a 'static' part and a 'parameter' part and then reassembling the PyTree after the transformation. This is an elegant way of solving the problem and is surprisingly powerful, because PyTrees can be filtered with arbitrary criteria (by default, jnp.array s are treated as parameters and everything else is treated as static). Here are some examples to get a feel for it: Works Breaks Works import jax import equinox as eqx class Exponentiate ( eqx . Module ): exponent : float @jax . jit def __call__ ( self , base : float ) -> float : return base ** self . exponent square = Exponentiate ( 2 ) assert square ( 2.0 ) == 4.0 # Works :) from typing import Callable import jax import equinox as eqx class ParameterisedOperation ( eqx . Module ): operation : Callable @jax . jit def __call__ ( self , operand : float ) -> float : return self . operation ( operand ) divide_by_two = ParameterisedOperation ( lambda x : x / 2 ) assert divide_by_two ( 2.0 ) == 1.0 # TypeError: Argument <function <lambda>...> is not a valid JAX type. from typing import Callable import jax import equinox as eqx class ParameterisedOperation ( eqx . Module ): operation : Callable @eqx . filter_jit ( default = eqx . is_array_like ) def __call__ ( self , operand : float ) -> float : return self . operation ( operand ) divide_by_two = ParameterisedOperation ( lambda x : x / 2 ) assert divide_by_two ( 2.0 ) == 1.0 # works :) The first example works because the self PyTree (consisting only of a single float) is JIT-able by JAX, so we can safely JIT the __call__() method. In the second example, however, the PyTree contains a Callable, which JAX does not know how to JIT. This is then fixed in the third example, where eqx.filter_jit automatically specifies the Callable as static. Info Notice that we used eqx.is_array_like as the filtering criterion. This is because the eqx.filter_jit default is to only trace jnp.array s, so the float operand would have been specified as static. This would have caused the function to recompile every time the operand changes. Tip If you are not sure whether your code is recompiling, put a print(\"compiling\") statement in your code. Side effects such as printing are only executed during compilation. Ideally, the statement will only be printed once. The most common place for this recompilation bug to occur is with the batch argument to train_step() and eval_step() in solstice.Experiment . Fortunately, this this bug is easy to catch because your performance will either be noticably poor, or you'll get errors for using your model with concrete values. Example Pseudo code for avoiding recompilation of train_step() class MyExperiment ( solstice . Experiment ): ... # always trace \"batch\" kwarg. eqx.filter_jit(default=eqx.is_array_like) would also work @eqx . filter_jit ( kwargs = dict ( batch = True )) def train_step ( self , batch : np . ndarray ): ... Filtered transformations are essentially the only caveat to Equinox, other than that, it's all pure JAX! To get started, you just need to remember to use equinox.filter_jit(kwargs=dict(batch=True)) when JIT-ing your train_step() and eval_step() . Once you've got the hang of it, you'll find other convenient uses such as freezing model parameters and more! If you prefer the Flax-style approach of specifying static fields up-front and then using pure jax.jit , you can avoid filtered transformations entirely by using equinox.static_field() : from typing import Callable import jax import equinox as eqx class ParameterisedOperation ( eqx . Module ): operation : Callable = eqx . static_field () @jax . jit def __call__ ( self , operand : float ) -> float : return self . operation ( operand ) divide_by_two = ParameterisedOperation ( lambda x : x / 2 ) assert divide_by_two ( 2.0 ) == 1.0 # works :)","title":"Primer - Background and Concepts"},{"location":"primer/#primer-background-and-concepts","text":"Here, we write about the concepts behind Solstice. It is not necessary to understand all of it before getting started, but it might help to understand our design.","title":"Primer - Background and Concepts"},{"location":"primer/#the-problem-coupling-in-ml-research-code","text":"In machine learning projects, you generally have at least four main parts of your code: model dataset training strategy (i.e. optimization, train/eval step logic, metrics calculation) training/testing loops (including logging, checkpointing etc...) In many research projects, it is helpful to be able to swap out any of these parts on a whim (ideally, just by changing your config files). In practice, it follows that the better you can decouple these four parts of your code, the faster you can iterate your experiments. Much attention is paid to the four components individually, but many researchers then just throw everything together haphazardly. It can require quite a significant engineering effort to properly decouple all the components, so most people don't bother. Wouldn't it be great if there was a standard way of organising your code to rapidly scale and iterate experiments...","title":"The Problem: coupling in ML research code"},{"location":"primer/#related-work-why-solstice","text":"We are not the first people to notice the usefulness of a well-engineered experiment library for deep learning. PyTorch Lightning has filled this niche for PyTorch and Sonnet/Keras have done the same for TensorFlow. In JAX, however, there is currently a cambrian explosion of libraries, e.g. Jaxline, Scenic, Objax, Elegy, CLU. All of these works are excellent, but they each have pros and cons. Libraries such as Jaxline, Objax, and Scenic are more 'framework-ish' - they ask you to do things their way and in return they do a lot for you. The natural tradeoff with this style of library is that you are locked in to their entire ecosystem which can result in a lack of flexibility. Of these, Elegy is the most closely related to Solstice due to its everything-is-a-pytree approach, however, where Elegy aims to be Keras-like, we aim to be lighter-weight. In Solstice, we leave nearly everything to the user and have incremental buy-in as a top design priority. CLU (for Flax) is also closely related, with Solstice having a similar Metrics API, as well as a similar philosophy of providing flexible utilities for common use cases. Surprisingly, Flax itself comes quite close to Solstice with its flax.training.TrainState concept. This is an object which holds experiment state with related functions and, taken to the extreme, can end up looking very similar to a solstice.Experiment . Equinox is the foundation of this project and is an elegant library for 'callable pytrees' in JAX (we explain this further below). Where Equinox strives to be as general as possible, we offer opinions about how to structure deep learning experiments. Solstice is a small library, implemented entirely in pure JAX+Equinox. Like Equinox, the simplicity of Solstice is its selling point.","title":"Related Work / Why Solstice?"},{"location":"primer/#the-key-idea-immutable-classes-in-python","text":"In object-oriented code, classes encapsulate both state and transformations on that state. Crucially, the state is mutable, so calling a method might return nothing but alter the object's state. This is a side effect . In functional programming, side effects are avoided by making state immutable, so instead of calling a method to update an object's state, you could pass the object to a pure function, returning a new object with the altered state. This is generally an easier paradigm to reason about and immutability is also needed in JAX for XLA to work its magic. This is all great, but Python is not really designed for the functional paradigm so it is difficult to fully decouple all the parts of your code. Type hinting functions with Protocols can get you surprisingly far, but at some point you will probably want to achieve some level of encapsulation and use abstract base classes to get dependency inversion. The approach we take in Solstice is to use immutable dataclasses to try to get the best of both worlds, the code below shows how you would implement a simple counter in each of the paradigms. OO-Style Functional-Style Solstice/Equinox-Style class OOPCounter : def __init__ ( self , initial_value : int = 0 ) -> None : self . count = initial_value def increment ( self ) -> None : self . count += 1 # 'initialise' the OO counter counter = OOPCounter () assert counter . count == 0 start_id = id ( counter ) # 'apply' the increment method counter . increment () assert counter . count == 1 end_id = id ( counter ) assert start_id == end_id def increment_fn ( current_value : int ) -> int : return current_value + 1 # 'initialise' the functional counter count = 0 assert count == 0 start_id = id ( count ) # 'apply' the increment func count = increment_fn ( count ) assert count == 1 end_id = id ( count ) assert start_id != end_id import dataclasses @dataclasses . dataclass ( frozen = True ) class SolsticeStyleCounter : count : int = 0 def increment ( self ) -> \"SolsticeStyleCounter\" : return dataclasses . replace ( self , count = self . count + 1 ) # 'initialise' the SolsticeStyleCounter counter = SolsticeStyleCounter () assert counter . count == 0 start_id = id ( counter ) # 'apply' the increment method, returning a new state object counter = counter . increment () assert counter . count == 1 end_id = id ( counter ) assert start_id != end_id Notice that the Solstice style counter did not mutate its state, it returned a new instance of itself with the new state. The great thing about this pattern is that by keeping our data structures immutable, we get to keep the readability and XLA optimization advantages that come with it and we also get all the power of Python classes and OO-ish design patterns. Warning The examples above use dataclasses.replace to perform out-of-place updates. This is a limited solution because it breaks when custom __init__() constructors are defined. Fortunately, Equinox has solved this problem with equinox.tree_at ; Solstice provides solstice.replace , a wrapped version of this function matching the dataclasses.replace syntax. See https://github.com/patrick-kidger/equinox/issues/120 for more details. In practice, in machine learning, this means we can replace the common init/apply pure functions with methods in a frozen dataclass (usually __init__() , and __call__() ). There is also one final matter to take care of... JAX only operates on PyTrees and doesn't know how to deal with dataclasses. This is why we build Solstice on top of Equinox, because an equinox.Module is just a dataclass which is registered as a PyTree. This is a powerful paradigm, and it allows us to trivially do things which are considerably more difficult in Flax/Haiku, like specifying common interfaces for models using abstract base classes: from abc import ABC , abstractmethod import equinox as eqx import jax.numpy as jnp class EncoderDecoderModel ( eqx . Module , ABC ): \"\"\"Encoder-Decoder models (e.g. VAEs) implement this interface.\"\"\" @abstractmethod def __init__ ( self ): \"\"\"Initialise model parameters.\"\"\" pass @abstractmethod def encode ( self , x : jnp . ndarray ) -> jnp . ndarray : \"\"\"Encode the input data into a latent representation.\"\"\" pass @abstractmethod def decode ( self , z : jnp . ndarray ) -> jnp . ndarray : \"\"\"Decode the latent representation.\"\"\" pass All classes in Solstice (except the training loop ones) inherit from equinox.Module . To get started, your mental model can simply be that an equinox.Module is a frozen dataclass that JAX can interpret, although it helps to understand the ideas behind Equinox a bit.","title":"The key idea: immutable classes in Python"},{"location":"primer/#equinox-and-filtered-transformations","text":"Filtered transformations are explained fully in the Equinox Docs , but we include a quick primer here for completeness. When we use Equinox to create class-style PyTrees, we are able to attach things to the PyTree that JAX function transformations don't know how to interpret. For example, you might store a callable, like your model's apply() function in a solstice.Experiment . Now, if you try to jax.jit a method in the experiment which takes self as a parameter, JAX with throw an error because it doesn't know how to JIT with callables as inputs to the function (usually, you would have to specify the argument as static). Equinox gets around this problem with filtered transformations, which involve splitting the PyTree into a 'static' part and a 'parameter' part and then reassembling the PyTree after the transformation. This is an elegant way of solving the problem and is surprisingly powerful, because PyTrees can be filtered with arbitrary criteria (by default, jnp.array s are treated as parameters and everything else is treated as static). Here are some examples to get a feel for it: Works Breaks Works import jax import equinox as eqx class Exponentiate ( eqx . Module ): exponent : float @jax . jit def __call__ ( self , base : float ) -> float : return base ** self . exponent square = Exponentiate ( 2 ) assert square ( 2.0 ) == 4.0 # Works :) from typing import Callable import jax import equinox as eqx class ParameterisedOperation ( eqx . Module ): operation : Callable @jax . jit def __call__ ( self , operand : float ) -> float : return self . operation ( operand ) divide_by_two = ParameterisedOperation ( lambda x : x / 2 ) assert divide_by_two ( 2.0 ) == 1.0 # TypeError: Argument <function <lambda>...> is not a valid JAX type. from typing import Callable import jax import equinox as eqx class ParameterisedOperation ( eqx . Module ): operation : Callable @eqx . filter_jit ( default = eqx . is_array_like ) def __call__ ( self , operand : float ) -> float : return self . operation ( operand ) divide_by_two = ParameterisedOperation ( lambda x : x / 2 ) assert divide_by_two ( 2.0 ) == 1.0 # works :) The first example works because the self PyTree (consisting only of a single float) is JIT-able by JAX, so we can safely JIT the __call__() method. In the second example, however, the PyTree contains a Callable, which JAX does not know how to JIT. This is then fixed in the third example, where eqx.filter_jit automatically specifies the Callable as static. Info Notice that we used eqx.is_array_like as the filtering criterion. This is because the eqx.filter_jit default is to only trace jnp.array s, so the float operand would have been specified as static. This would have caused the function to recompile every time the operand changes. Tip If you are not sure whether your code is recompiling, put a print(\"compiling\") statement in your code. Side effects such as printing are only executed during compilation. Ideally, the statement will only be printed once. The most common place for this recompilation bug to occur is with the batch argument to train_step() and eval_step() in solstice.Experiment . Fortunately, this this bug is easy to catch because your performance will either be noticably poor, or you'll get errors for using your model with concrete values. Example Pseudo code for avoiding recompilation of train_step() class MyExperiment ( solstice . Experiment ): ... # always trace \"batch\" kwarg. eqx.filter_jit(default=eqx.is_array_like) would also work @eqx . filter_jit ( kwargs = dict ( batch = True )) def train_step ( self , batch : np . ndarray ): ... Filtered transformations are essentially the only caveat to Equinox, other than that, it's all pure JAX! To get started, you just need to remember to use equinox.filter_jit(kwargs=dict(batch=True)) when JIT-ing your train_step() and eval_step() . Once you've got the hang of it, you'll find other convenient uses such as freezing model parameters and more! If you prefer the Flax-style approach of specifying static fields up-front and then using pure jax.jit , you can avoid filtered transformations entirely by using equinox.static_field() : from typing import Callable import jax import equinox as eqx class ParameterisedOperation ( eqx . Module ): operation : Callable = eqx . static_field () @jax . jit def __call__ ( self , operand : float ) -> float : return self . operation ( operand ) divide_by_two = ParameterisedOperation ( lambda x : x / 2 ) assert divide_by_two ( 2.0 ) == 1.0 # works :)","title":"Equinox and filtered transformations"},{"location":"api/solstice/","text":"Solstice, a library for creating and scaling experiments in JAX. Whole API \u00a4 Abstract This is all of Solstice. Everything is accessible through the solstice.* namespace. solstice . __all__ = ( 'Experiment' , 'Metrics' , 'ClassificationMetrics' , 'Callback' , 'CheckpointingCallback' , 'EarlyStoppingCallback' , 'LoggingCallback' , 'ProfilingCallback' , 'train' , 'test' , 'replace' , 'EarlyStoppingException' ) module-attribute \u00a4 \u00a4 Experiments \u00a4 The Experiment is at the heart of Solstice. The API is similar to the pl.LightningModule loved by PyTorch-Lightning users, but we do less 'magic' to keep it as transparent as possible. If in doubt, just read the source code - it's really short! Experiment \u00a4 Bases: eqx . Module , ABC Base class for Solstice experiments. An Experiment holds all stateful models, optimizers, etc... for a run and implements this interface. To make your own experiments, subclass this class and implement the logic for initialisation, training, and evaluating. Tip This is a subclass of equinox.Module , so you are free to use pure JAX transformations such as jax.jit and jax.pmap , as long as you remember to filter out static PyTree fields (e.g. with eqx.filter_jit ). Example Pseudocode for typical Experiment usage: exp = MyExperiment ( ... ) # initialise experiment state for step in range ( num_steps ): exp , outs = exp . train_step ( batch ) #do anything with the outputs here # exp is just a pytree, so we can save and restore checkpoints like so... equinox . tree_serialise_leaves ( \"checkpoint_0.eqx\" , exp ) This class just specifies a recommended interface for experiment code. Experiments implementing this interface will automatically work with the Solstice training loops. You can always create or override methods as you wish and no methods are special-cased. For example it is common to define a __call__ method to perform inference on a batch of data. Source code in solstice/experiment.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class Experiment ( eqx . Module , ABC ): \"\"\"Base class for Solstice experiments. An Experiment holds all stateful models, optimizers, etc... for a run and implements this interface. To make your own experiments, subclass this class and implement the logic for initialisation, training, and evaluating. !!! tip This is a subclass of `equinox.Module`, so you are free to use pure JAX transformations such as `jax.jit` and `jax.pmap`, as long as you remember to filter out static PyTree fields (e.g. with `eqx.filter_jit`). !!! example Pseudocode for typical `Experiment` usage: ```python exp = MyExperiment(...) # initialise experiment state for step in range(num_steps): exp, outs = exp.train_step(batch) #do anything with the outputs here # exp is just a pytree, so we can save and restore checkpoints like so... equinox.tree_serialise_leaves(\"checkpoint_0.eqx\", exp) ``` This class just specifies a recommended interface for experiment code. Experiments implementing this interface will automatically work with the Solstice training loops. You can always create or override methods as you wish and no methods are special-cased. For example it is common to define a `__call__` method to perform inference on a batch of data. \"\"\" @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialise the experiment. !!! example Pseudocode implementation for initialising an MNIST classifier with flax and optax: ```python class MNISTExperiment(Experiment): params: Any opt_state: Any opt_apply: Callable model_apply: Callable num_classes: int def __init__(self, rng: int, model: flax.nn.Module, optimizer = optax.GradientTransformation ) -> None: key = jax.random.PRNGKey(rng) dummy_batch = jnp.zeros((32, 784)) self.params = model.init(key, dummy_batch) self.model_apply = model.apply self.opt = optax.adam(learning_rate=1e-3) self.opt_state = optimizer.init(self.params) self.num_classes = 10 ``` \"\"\" raise NotImplementedError () @abstractmethod def train_step ( self , batch : Any ) -> Tuple [ Experiment , Any ]: \"\"\"A training step takes a batch of data and returns the updated experiment and any auxiliary outputs (usually a `solstice.Metrics` object). !!! tip You will typically want to use `jax.jit`, `jax.pmap`, `eqx.filter_jit`, or `eqx.filter_pmap` on this method. See the [solstice primer](https://charl-ai.github.io/Solstice/primer/) for more info on filtered transformations. You can also read the tutorial on different [parallelism strategies](https://charl-ai.github.io/Solstice/parallelism_strategies/). !!! example Pseudocode implementation of a training step: ```python class MNISTExperiment(Experiment): @eqx.filter_jit(kwargs=dict(batch=True)) def train_step(self, batch: Tuple[np.ndarray, ...] ) -> Tuple[Experiment, solstice.Metrics]: imgs, labels = batch def loss_fn(params, x, y): ... # compute loss return loss, logits (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)( self.params, imgs, labels ) new_params, new_opt_state = ... # calculate grads and update params preds = jnp.argmax(logits, axis=-1) metrics = MyMetrics(preds, labels, loss) return ( solstice.replace(self, params=new_params, opt_state=new_opt_state), metrics, ) ``` !!! tip You can use the `solstice.replace` function as a way of returning an experiment instance with modified state. Args: batch (Any): Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. \"\"\" raise NotImplementedError () @abstractmethod def eval_step ( self , batch : Any ) -> Tuple [ Experiment , Any ]: \"\"\"An evaluation step (e.g. for validation or testing) takes a batch of data and returns the updated experiment and any auxiliary outputs. Usually, this will be a `solstice.Metrics` object. Like `train_step()`, you should probably JIT this method. !!! tip In most evaluation cases, the experiment returned will be unchanged, the main reason why you would want to modify it is to advance PRNG state. !!! example Pseudocode implementation of an evaluation step: ```python class MNISTExperiment(Experiment): @eqx.filter_jit(kwargs=dict(batch=True)) def eval_step(self, batch: Tuple[np.ndarray, ...] ) -> Tuple[Experiment, Any]: imgs, labels = batch logits = ... # apply the model e.g. self.apply_fn(imgs) loss = ... # compute loss preds = jnp.argmax(logits, axis=-1) metrics = MyMetrics(preds, labels, loss) return self, metrics ``` Args: batch (Any): Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. \"\"\" raise NotImplementedError () __init__ ( * args , ** kwargs ) -> None abstractmethod \u00a4 Initialise the experiment. Example Pseudocode implementation for initialising an MNIST classifier with flax and optax: class MNISTExperiment ( Experiment ): params : Any opt_state : Any opt_apply : Callable model_apply : Callable num_classes : int def __init__ ( self , rng : int , model : flax . nn . Module , optimizer = optax . GradientTransformation ) -> None : key = jax . random . PRNGKey ( rng ) dummy_batch = jnp . zeros (( 32 , 784 )) self . params = model . init ( key , dummy_batch ) self . model_apply = model . apply self . opt = optax . adam ( learning_rate = 1e-3 ) self . opt_state = optimizer . init ( self . params ) self . num_classes = 10 Source code in solstice/experiment.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialise the experiment. !!! example Pseudocode implementation for initialising an MNIST classifier with flax and optax: ```python class MNISTExperiment(Experiment): params: Any opt_state: Any opt_apply: Callable model_apply: Callable num_classes: int def __init__(self, rng: int, model: flax.nn.Module, optimizer = optax.GradientTransformation ) -> None: key = jax.random.PRNGKey(rng) dummy_batch = jnp.zeros((32, 784)) self.params = model.init(key, dummy_batch) self.model_apply = model.apply self.opt = optax.adam(learning_rate=1e-3) self.opt_state = optimizer.init(self.params) self.num_classes = 10 ``` \"\"\" raise NotImplementedError () train_step ( batch : Any ) -> Tuple [ Experiment , Any ] abstractmethod \u00a4 A training step takes a batch of data and returns the updated experiment and any auxiliary outputs (usually a solstice.Metrics object). Tip You will typically want to use jax.jit , jax.pmap , eqx.filter_jit , or eqx.filter_pmap on this method. See the solstice primer for more info on filtered transformations. You can also read the tutorial on different parallelism strategies . Example Pseudocode implementation of a training step: class MNISTExperiment ( Experiment ): @eqx . filter_jit ( kwargs = dict ( batch = True )) def train_step ( self , batch : Tuple [ np . ndarray , ... ] ) -> Tuple [ Experiment , solstice . Metrics ]: imgs , labels = batch def loss_fn ( params , x , y ): ... # compute loss return loss , logits ( loss , logits ), grads = jax . value_and_grad ( loss_fn , has_aux = True )( self . params , imgs , labels ) new_params , new_opt_state = ... # calculate grads and update params preds = jnp . argmax ( logits , axis =- 1 ) metrics = MyMetrics ( preds , labels , loss ) return ( solstice . replace ( self , params = new_params , opt_state = new_opt_state ), metrics , ) Tip You can use the solstice.replace function as a way of returning an experiment instance with modified state. Parameters: batch ( Any ) \u2013 Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple [ Experiment , Any ] \u2013 Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. Source code in solstice/experiment.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 @abstractmethod def train_step ( self , batch : Any ) -> Tuple [ Experiment , Any ]: \"\"\"A training step takes a batch of data and returns the updated experiment and any auxiliary outputs (usually a `solstice.Metrics` object). !!! tip You will typically want to use `jax.jit`, `jax.pmap`, `eqx.filter_jit`, or `eqx.filter_pmap` on this method. See the [solstice primer](https://charl-ai.github.io/Solstice/primer/) for more info on filtered transformations. You can also read the tutorial on different [parallelism strategies](https://charl-ai.github.io/Solstice/parallelism_strategies/). !!! example Pseudocode implementation of a training step: ```python class MNISTExperiment(Experiment): @eqx.filter_jit(kwargs=dict(batch=True)) def train_step(self, batch: Tuple[np.ndarray, ...] ) -> Tuple[Experiment, solstice.Metrics]: imgs, labels = batch def loss_fn(params, x, y): ... # compute loss return loss, logits (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)( self.params, imgs, labels ) new_params, new_opt_state = ... # calculate grads and update params preds = jnp.argmax(logits, axis=-1) metrics = MyMetrics(preds, labels, loss) return ( solstice.replace(self, params=new_params, opt_state=new_opt_state), metrics, ) ``` !!! tip You can use the `solstice.replace` function as a way of returning an experiment instance with modified state. Args: batch (Any): Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. \"\"\" raise NotImplementedError () eval_step ( batch : Any ) -> Tuple [ Experiment , Any ] abstractmethod \u00a4 An evaluation step (e.g. for validation or testing) takes a batch of data and returns the updated experiment and any auxiliary outputs. Usually, this will be a solstice.Metrics object. Like train_step() , you should probably JIT this method. Tip In most evaluation cases, the experiment returned will be unchanged, the main reason why you would want to modify it is to advance PRNG state. Example Pseudocode implementation of an evaluation step: class MNISTExperiment ( Experiment ): @eqx . filter_jit ( kwargs = dict ( batch = True )) def eval_step ( self , batch : Tuple [ np . ndarray , ... ] ) -> Tuple [ Experiment , Any ]: imgs , labels = batch logits = ... # apply the model e.g. self.apply_fn(imgs) loss = ... # compute loss preds = jnp . argmax ( logits , axis =- 1 ) metrics = MyMetrics ( preds , labels , loss ) return self , metrics Parameters: batch ( Any ) \u2013 Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple [ Experiment , Any ] \u2013 Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. Source code in solstice/experiment.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @abstractmethod def eval_step ( self , batch : Any ) -> Tuple [ Experiment , Any ]: \"\"\"An evaluation step (e.g. for validation or testing) takes a batch of data and returns the updated experiment and any auxiliary outputs. Usually, this will be a `solstice.Metrics` object. Like `train_step()`, you should probably JIT this method. !!! tip In most evaluation cases, the experiment returned will be unchanged, the main reason why you would want to modify it is to advance PRNG state. !!! example Pseudocode implementation of an evaluation step: ```python class MNISTExperiment(Experiment): @eqx.filter_jit(kwargs=dict(batch=True)) def eval_step(self, batch: Tuple[np.ndarray, ...] ) -> Tuple[Experiment, Any]: imgs, labels = batch logits = ... # apply the model e.g. self.apply_fn(imgs) loss = ... # compute loss preds = jnp.argmax(logits, axis=-1) metrics = MyMetrics(preds, labels, loss) return self, metrics ``` Args: batch (Any): Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. \"\"\" raise NotImplementedError () Metrics \u00a4 Our Metrics API is similar to the one in CLU , although more sexy because we use equinox :) We favour defining one single object for handling all metrics for an experiment instead of composing multiple objects into a collection. This is more efficient because often we can calculate a battery of metrics from the same intermediate results. It is also simpler and easier to reason about. Metrics \u00a4 Bases: eqx . Module , ABC Base class for metrics. A Metrics object handles calculating intermediate metrics from model outputs, accumulating them over batches, then calculating final metrics from accumulated metrics. Subclass this class and implement the interface for initialisation, accumulation, and finalisation. Tip This class doesn't have to handle 'metrics' in the strictest sense. You could implement a Metrics class to collect output images for plotting for example. Example Pseudocode for typical Metrics usage: metrics = None for batch in dataset : batch_metrics = step ( batch ) # step returns a Metrics object metrics = metrics . merge ( batch_metrics ) if metrics else batch_metrics if time_to_log : metrics_dict = metrics . compute () ... # log your metrics here metrics = None # reset the object Source code in solstice/metrics.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class Metrics ( eqx . Module , ABC ): \"\"\"Base class for metrics. A Metrics object handles calculating intermediate metrics from model outputs, accumulating them over batches, then calculating final metrics from accumulated metrics. Subclass this class and implement the interface for initialisation, accumulation, and finalisation. !!! tip This class doesn't have to handle 'metrics' in the strictest sense. You could implement a `Metrics` class to collect output images for plotting for example. !!! example Pseudocode for typical `Metrics` usage: ```python metrics = None for batch in dataset: batch_metrics = step(batch) # step returns a Metrics object metrics = metrics.merge(batch_metrics) if metrics else batch_metrics if time_to_log: metrics_dict = metrics.compute() ... # log your metrics here metrics = None # reset the object ``` \"\"\" @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialise a metrics object, typically with predictions and targets. !!! example Pseudocode for typical `Metrics` initialisation, this example object will keep track of the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): count: int num_correct: int def __init__(self, preds: jnp.ndarray, targets: jnp.ndarray) -> None: self.count = preds.shape[0] # assumes batch is first dim self.num_correct = jnp.sum(preds == targets) ``` !!! tip In classification settings, the confusion matrix is a useful intermediate result to calculate during initialisation. \"\"\" raise NotImplementedError @abstractmethod def merge ( self , other : Metrics ) -> Metrics : \"\"\"Merge two metrics objects, returning a new metrics object. !!! example Pseudocode for typical `Metrics` merging, in the example code, we can simply sum the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): def merge(self, other: Metrics) -> Metrics: new_num_correct = self.num_correct + other.num_correct new_count = self.count + other.count return solstice.replace(self, num_correct=new_num_correct, count=new_count) ``` \"\"\" raise NotImplementedError @abstractmethod def compute ( self ) -> Any : \"\"\"Compute final metrics from accumulated metrics. !!! example Pseudocode for typical `Metrics` finalisation, here we calculate accuracy from the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): def compute(self) -> Mapping[str, float]: return {'accuracy': self.num_correct / self.count} ``` !!! tip Typically, you will want to return a dictionary of metrics. Try to put any expensive computations here, not in `__init__`. \"\"\" raise NotImplementedError __init__ ( * args , ** kwargs ) -> None abstractmethod \u00a4 Initialise a metrics object, typically with predictions and targets. Example Pseudocode for typical Metrics initialisation, this example object will keep track of the number of correct predictions and the total number of predictions: class MyMetrics ( Metrics ): count : int num_correct : int def __init__ ( self , preds : jnp . ndarray , targets : jnp . ndarray ) -> None : self . count = preds . shape [ 0 ] # assumes batch is first dim self . num_correct = jnp . sum ( preds == targets ) Tip In classification settings, the confusion matrix is a useful intermediate result to calculate during initialisation. Source code in solstice/metrics.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialise a metrics object, typically with predictions and targets. !!! example Pseudocode for typical `Metrics` initialisation, this example object will keep track of the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): count: int num_correct: int def __init__(self, preds: jnp.ndarray, targets: jnp.ndarray) -> None: self.count = preds.shape[0] # assumes batch is first dim self.num_correct = jnp.sum(preds == targets) ``` !!! tip In classification settings, the confusion matrix is a useful intermediate result to calculate during initialisation. \"\"\" raise NotImplementedError merge ( other : Metrics ) -> Metrics abstractmethod \u00a4 Merge two metrics objects, returning a new metrics object. Example Pseudocode for typical Metrics merging, in the example code, we can simply sum the number of correct predictions and the total number of predictions: class MyMetrics ( Metrics ): def merge ( self , other : Metrics ) -> Metrics : new_num_correct = self . num_correct + other . num_correct new_count = self . count + other . count return solstice . replace ( self , num_correct = new_num_correct , count = new_count ) Source code in solstice/metrics.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @abstractmethod def merge ( self , other : Metrics ) -> Metrics : \"\"\"Merge two metrics objects, returning a new metrics object. !!! example Pseudocode for typical `Metrics` merging, in the example code, we can simply sum the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): def merge(self, other: Metrics) -> Metrics: new_num_correct = self.num_correct + other.num_correct new_count = self.count + other.count return solstice.replace(self, num_correct=new_num_correct, count=new_count) ``` \"\"\" raise NotImplementedError compute () -> Any abstractmethod \u00a4 Compute final metrics from accumulated metrics. Example Pseudocode for typical Metrics finalisation, here we calculate accuracy from the number of correct predictions and the total number of predictions: class MyMetrics ( Metrics ): def compute ( self ) -> Mapping [ str , float ]: return { 'accuracy' : self . num_correct / self . count } Tip Typically, you will want to return a dictionary of metrics. Try to put any expensive computations here, not in __init__ . Source code in solstice/metrics.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @abstractmethod def compute ( self ) -> Any : \"\"\"Compute final metrics from accumulated metrics. !!! example Pseudocode for typical `Metrics` finalisation, here we calculate accuracy from the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): def compute(self) -> Mapping[str, float]: return {'accuracy': self.num_correct / self.count} ``` !!! tip Typically, you will want to return a dictionary of metrics. Try to put any expensive computations here, not in `__init__`. \"\"\" raise NotImplementedError ClassificationMetrics \u00a4 Bases: Metrics Basic metrics for multiclass classification tasks. Metrics included: Average Loss Accuracy Prevalence F1 score Sensitivity (TPR, recall) Positive predictive value (PPV, precision) Accuracy is reported as Top-1 accuracy which is equal to the micro-average of precision/recall/f1. Prevalence is reported on a per-class basis. Precision, Recall and F1 are reported three times: per-class, macro-average, and weighted average (by prevalence). Not for multi-label classification. Info See https://en.wikipedia.org/wiki/Confusion_matrix for more on confusion matrices and classification metrics. See https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel for more on multiclass micro/macro/weighted averaging. Source code in solstice/metrics.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 class ClassificationMetrics ( Metrics ): \"\"\"Basic metrics for multiclass classification tasks. !!! summary \"Metrics included:\" - Average Loss - Accuracy - Prevalence - F1 score - Sensitivity (TPR, recall) - Positive predictive value (PPV, precision) Accuracy is reported as Top-1 accuracy which is equal to the micro-average of precision/recall/f1. Prevalence is reported on a per-class basis. Precision, Recall and F1 are reported three times: per-class, macro-average, and weighted average (by prevalence). *Not* for multi-label classification. !!! info See https://en.wikipedia.org/wiki/Confusion_matrix for more on confusion matrices and classification metrics. See https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel for more on multiclass micro/macro/weighted averaging. \"\"\" _confusion_matrix : jnp . ndarray _average_loss : float _count : int _num_classes : int def __init__ ( self , preds : jnp . ndarray , targets : jnp . ndarray , loss : float , num_classes : int ) -> None : \"\"\" Create a ClassificationMetrics object from model predictions and targets. Args: preds (jnp.ndarray): Non OH encoded predictions, shape: (batch_size,). targets (jnp.ndarray): Non OH encoded targets, shape: (batch_size,). loss (float): Average loss over the batch (scalar). num_classes (int): Number of classes in classification problem. \"\"\" self . _confusion_matrix = _compute_confusion_matrix ( preds , targets , num_classes ) self . _average_loss = loss self . _count = preds . shape [ 0 ] self . _num_classes = num_classes def merge ( self , other : ClassificationMetrics ) -> ClassificationMetrics : assert isinstance ( other , ClassificationMetrics ), ( \"Can only merge ClassificationMetrics object with another\" f \" ClassificationMetrics object, got { type ( other ) } \" ) assert self . _num_classes == other . _num_classes , ( \"Can only merge metrics with same num_classes, got\" f \" { self . _num_classes } and { other . _num_classes } \" ) # can simply sum confusion matrices and count new_cm = self . _confusion_matrix + other . _confusion_matrix new_count = self . _count + other . _count # average loss is weighted by count from each object new_loss = ( self . _average_loss * self . _count + other . _average_loss * other . _count ) / ( self . _count + other . _count ) return replace ( self , _confusion_matrix = new_cm , _average_loss = new_loss , _count = new_count ) def compute ( self ) -> Mapping [ str , float ]: metrics = _compute_metrics_from_cm ( self . _confusion_matrix ) metrics [ \"average_loss\" ] = self . _average_loss return metrics __init__ ( preds : jnp . ndarray , targets : jnp . ndarray , loss : float , num_classes : int ) -> None \u00a4 Create a ClassificationMetrics object from model predictions and targets. Parameters: preds ( jnp . ndarray ) \u2013 Non OH encoded predictions, shape: (batch_size,). targets ( jnp . ndarray ) \u2013 Non OH encoded targets, shape: (batch_size,). loss ( float ) \u2013 Average loss over the batch (scalar). num_classes ( int ) \u2013 Number of classes in classification problem. Source code in solstice/metrics.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def __init__ ( self , preds : jnp . ndarray , targets : jnp . ndarray , loss : float , num_classes : int ) -> None : \"\"\" Create a ClassificationMetrics object from model predictions and targets. Args: preds (jnp.ndarray): Non OH encoded predictions, shape: (batch_size,). targets (jnp.ndarray): Non OH encoded targets, shape: (batch_size,). loss (float): Average loss over the batch (scalar). num_classes (int): Number of classes in classification problem. \"\"\" self . _confusion_matrix = _compute_confusion_matrix ( preds , targets , num_classes ) self . _average_loss = loss self . _count = preds . shape [ 0 ] self . _num_classes = num_classes Training \u00a4 Training loops are usually boilerplate code that has little to do with your research. We provide training and testing loops which integrate with a simple and flexible callback system. Any solstice.Experiment can be passed to the loops, but you can always write your own if necessary. We provide a handful of pre-implemented callbacks, but if they do not suit your needs, you can use them as inspiration to write your own. Callback \u00a4 Bases: ABC Base class for callbacks to solstice.train() and `solstice.test(). Subclass and implement this interface to inject arbitrary functionality into the training and testing loops. Tip All callback hooks return None , so they cannot affect the training itself. Use callbacks to execute side effects like logging, checkpointing or profiling. Example Pseudocode callback implementation for logging with solstice.Metrics : class MyLoggingCallback ( Callback ): def __init__ ( self , log_every_n_steps , ... ): self . metrics = None self . log_every_n_steps = log_every_n_steps ... # set up logging, e.g. wandb.init(...) def on_step_end ( self , exp , global_step , training , batch , outs ): assert isinstance ( outs , solstice . Metrics ) self . metrics = outs . merge ( self . metrics ) if self . metrics else outs if ( global_step + 1 ) % self . log_every_n_steps == 0 : metrics_dict = self . metrics . compute () ... # do logging e.g. wandb.log(metrics_dict) self . metrics = None Source code in solstice/trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class Callback ( ABC ): \"\"\"Base class for callbacks to `solstice.train()` and `solstice.test(). Subclass and implement this interface to inject arbitrary functionality into the training and testing loops. !!! tip All callback hooks return `None`, so they cannot affect the training itself. Use callbacks to execute side effects like logging, checkpointing or profiling. !!! example Pseudocode callback implementation for logging with `solstice.Metrics`: ```python class MyLoggingCallback(Callback): def __init__(self, log_every_n_steps, ...): self.metrics = None self.log_every_n_steps = log_every_n_steps ... # set up logging, e.g. wandb.init(...) def on_step_end(self, exp, global_step, training, batch, outs): assert isinstance(outs, solstice.Metrics) self.metrics = outs.merge(self.metrics) if self.metrics else outs if (global_step + 1) % self.log_every_n_steps == 0: metrics_dict = self.metrics.compute() ... # do logging e.g. wandb.log(metrics_dict) self.metrics = None ``` \"\"\" @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialize the callback.\"\"\" raise NotImplementedError def on_epoch_start ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : \"\"\"Called at the start of each epoch, i.e. before the model has seen any data for that epoch. Args: exp (Experiment): Current Experiment state. epoch (int): Current epoch number. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing epoch. \"\"\" pass def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : \"\"\"Called at the end of each epoch, i.e. after the model has seen the full dataset for that epoch. Args: exp (Experiment): Current Experiment state. epoch (int): Current epoch number. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. \"\"\" pass def on_step_start ( self , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : \"\"\"Called at the start of each training and validation step, i.e. before the batch has been seen. Args: exp (Experiment): Current Experiment state. global_step (int): Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. batch (Any): Current batch of data for this step. \"\"\" pass def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : \"\"\"Called at the end of each training and validation step, i.e. after the batch has been seen. Args: exp (Experiment): Current Experiment state. global_step (int): Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. batch (Any): Current batch of data for this step. outs (Any): Auxiliary outputs from the experiment train/eval step. Usually, this should be a `solstice.Metrics` object. \"\"\" pass __init__ ( * args , ** kwargs ) -> None abstractmethod \u00a4 Initialize the callback. Source code in solstice/trainer.py 59 60 61 62 @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialize the callback.\"\"\" raise NotImplementedError on_epoch_start ( exp : Experiment , epoch : int , mode : Literal [ train , val , test ]) -> None \u00a4 Called at the start of each epoch, i.e. before the model has seen any data for that epoch. Parameters: exp ( Experiment ) \u2013 Current Experiment state. epoch ( int ) \u2013 Current epoch number. mode ( Literal [ train , val , test ] ) \u2013 String representing whether this is a training, validation or testing epoch. Source code in solstice/trainer.py 64 65 66 67 68 69 70 71 72 73 74 75 76 def on_epoch_start ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : \"\"\"Called at the start of each epoch, i.e. before the model has seen any data for that epoch. Args: exp (Experiment): Current Experiment state. epoch (int): Current epoch number. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing epoch. \"\"\" pass on_epoch_end ( exp : Experiment , epoch : int , mode : Literal [ train , val , test ]) -> None \u00a4 Called at the end of each epoch, i.e. after the model has seen the full dataset for that epoch. Parameters: exp ( Experiment ) \u2013 Current Experiment state. epoch ( int ) \u2013 Current epoch number. mode ( Literal [ train , val , test ] ) \u2013 String representing whether this is a training, validation or testing step. Source code in solstice/trainer.py 78 79 80 81 82 83 84 85 86 87 88 89 90 def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : \"\"\"Called at the end of each epoch, i.e. after the model has seen the full dataset for that epoch. Args: exp (Experiment): Current Experiment state. epoch (int): Current epoch number. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. \"\"\" pass on_step_start ( exp : Experiment , global_step : int , mode : Literal [ train , val , test ], batch : Any ) -> None \u00a4 Called at the start of each training and validation step, i.e. before the batch has been seen. Parameters: exp ( Experiment ) \u2013 Current Experiment state. global_step ( int ) \u2013 Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode ( Literal [ train , val , test ] ) \u2013 String representing whether this is a training, validation or testing step. batch ( Any ) \u2013 Current batch of data for this step. Source code in solstice/trainer.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def on_step_start ( self , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : \"\"\"Called at the start of each training and validation step, i.e. before the batch has been seen. Args: exp (Experiment): Current Experiment state. global_step (int): Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. batch (Any): Current batch of data for this step. \"\"\" pass on_step_end ( outs : Any , exp : Experiment , global_step : int , mode : Literal [ train , val , test ], batch : Any ) -> None \u00a4 Called at the end of each training and validation step, i.e. after the batch has been seen. Parameters: exp ( Experiment ) \u2013 Current Experiment state. global_step ( int ) \u2013 Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode ( Literal [ train , val , test ] ) \u2013 String representing whether this is a training, validation or testing step. batch ( Any ) \u2013 Current batch of data for this step. outs ( Any ) \u2013 Auxiliary outputs from the experiment train/eval step. Usually, this should be a solstice.Metrics object. Source code in solstice/trainer.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : \"\"\"Called at the end of each training and validation step, i.e. after the batch has been seen. Args: exp (Experiment): Current Experiment state. global_step (int): Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. batch (Any): Current batch of data for this step. outs (Any): Auxiliary outputs from the experiment train/eval step. Usually, this should be a `solstice.Metrics` object. \"\"\" pass LoggingCallback \u00a4 Bases: Callback Logs auxiliary outputs from training or evaulation steps (either periodically every n steps, or at the end of the epoch). Internally, this accumulates metrics with metrics.merge() , computes them with metrics.compute() , and then passes the final results to the given logging function. Warning Auxiliary outputs from the train and eval steps must be a solstice.Metrics instance for this callback to work properly. We raise an AssertionError if this is not the case. Note There are many different libraries you can use for writing logs (e.g. wandb, TensorBoard(X), ...). We offer no opinion on which one you should use. Pass in a logging function to use any arbitrary logger. Source code in solstice/trainer.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 class LoggingCallback ( Callback ): \"\"\"Logs auxiliary outputs from training or evaulation steps (either periodically every n steps, or at the end of the epoch). Internally, this accumulates metrics with `metrics.merge()`, computes them with `metrics.compute()`, and then passes the final results to the given logging function. !!! warning Auxiliary outputs from the train and eval steps must be a `solstice.Metrics` instance for this callback to work properly. We raise an AssertionError if this is not the case. !!! note There are many different libraries you can use for writing logs (e.g. wandb, TensorBoard(X), ...). We offer no opinion on which one you should use. Pass in a logging function to use any arbitrary logger. \"\"\" def __init__ ( self , log_every_n_steps : int | None = None , logging_fn : Callable [[ Any , int , Literal [ \"train\" , \"val\" , \"test\" ]], None ] | None = None , ) -> None : \"\"\"Initialize the logging callback. Args: log_every_n_steps (int | None, optional): If given, accumulate metrics over n steps before logging. If None, log at end of epoch. Defaults to None. logging_fn (Callable[[Any, int, Literal['train', 'val', 'test']], None] | None, optional): Logging function. Takes the outputs of `metrics.compute()`, the current step or epoch number, and a string representing whether training, validating, or testing. The function should return nothing. If no logging_fn is given, the default behaviour is to log with the built in Python logger (INFO level). Defaults to None. !!! example The default logging function (used if None is given) logs using the built in Python logger, with name \"solstice\" and INFO level (notice that the output of `metrics.compute()` must be printable): ```python logger = logging.getLogger(\"solstice\") default_logger = lambda metrics, step, mode: logging.info( f\"{mode} step {step}: {metrics}\" ) ``` If the logs aren't showing, you might need to put this line at the top of your script: ```python import logging logging.getLogger(\"solstice\").setLevel(logging.INFO) ``` \"\"\" default_logger = lambda metrics , step , mode : logger . info ( f \" { mode } step { step } : { metrics } \" ) self . logging_fn = logging_fn if logging_fn else default_logger self . log_every_n_steps = log_every_n_steps self . metrics = None def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : del exp , batch assert isinstance ( outs , Metrics ) self . metrics = outs . merge ( self . metrics ) if self . metrics else outs if self . log_every_n_steps and ( global_step + 1 ) % self . log_every_n_steps == 0 : final_metrics = self . metrics . compute () self . logging_fn ( final_metrics , global_step , mode ) self . metrics = None def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : del exp # if not logging every n steps, we just log at the end of the epoch if not self . log_every_n_steps : assert self . metrics is not None final_metrics = self . metrics . compute () self . logging_fn ( final_metrics , epoch , mode ) # reset the metrics object to prevent train/val metrics from being mixed self . metrics = None __init__ ( log_every_n_steps : int | None = None , logging_fn : Callable [[ Any , int , Literal [ train , val , test ]], None ] | None = None ) -> None \u00a4 Initialize the logging callback. Parameters: log_every_n_steps ( int | None ) \u2013 If given, accumulate metrics over n steps before logging. If None, log at end of epoch. Defaults to None. logging_fn ( Callable [[ Any , int , Literal [ train , val , test ]], None] | None ) \u2013 Logging function. Takes the outputs of metrics.compute() , the current step or epoch number, and a string representing whether training, validating, or testing. The function should return nothing. If no logging_fn is given, the default behaviour is to log with the built in Python logger (INFO level). Defaults to None. Example The default logging function (used if None is given) logs using the built in Python logger, with name \"solstice\" and INFO level (notice that the output of metrics.compute() must be printable): logger = logging . getLogger ( \"solstice\" ) default_logger = lambda metrics , step , mode : logging . info ( f \" { mode } step { step } : { metrics } \" ) If the logs aren't showing, you might need to put this line at the top of your script: import logging logging . getLogger ( \"solstice\" ) . setLevel ( logging . INFO ) Source code in solstice/trainer.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def __init__ ( self , log_every_n_steps : int | None = None , logging_fn : Callable [[ Any , int , Literal [ \"train\" , \"val\" , \"test\" ]], None ] | None = None , ) -> None : \"\"\"Initialize the logging callback. Args: log_every_n_steps (int | None, optional): If given, accumulate metrics over n steps before logging. If None, log at end of epoch. Defaults to None. logging_fn (Callable[[Any, int, Literal['train', 'val', 'test']], None] | None, optional): Logging function. Takes the outputs of `metrics.compute()`, the current step or epoch number, and a string representing whether training, validating, or testing. The function should return nothing. If no logging_fn is given, the default behaviour is to log with the built in Python logger (INFO level). Defaults to None. !!! example The default logging function (used if None is given) logs using the built in Python logger, with name \"solstice\" and INFO level (notice that the output of `metrics.compute()` must be printable): ```python logger = logging.getLogger(\"solstice\") default_logger = lambda metrics, step, mode: logging.info( f\"{mode} step {step}: {metrics}\" ) ``` If the logs aren't showing, you might need to put this line at the top of your script: ```python import logging logging.getLogger(\"solstice\").setLevel(logging.INFO) ``` \"\"\" default_logger = lambda metrics , step , mode : logger . info ( f \" { mode } step { step } : { metrics } \" ) self . logging_fn = logging_fn if logging_fn else default_logger self . log_every_n_steps = log_every_n_steps self . metrics = None CheckpointingCallback \u00a4 Bases: Callback Checkpoint the experiment state at the end of each epoch. Todo Implement this. Consider adding asynchronous checkpointing. Source code in solstice/trainer.py 231 232 233 234 235 236 237 class CheckpointingCallback ( Callback ): \"\"\"Checkpoint the experiment state at the end of each epoch. !!! todo Implement this. Consider adding asynchronous checkpointing.\"\"\" pass ProfilingCallback \u00a4 Bases: Callback Uses the built-in JAX (TensorBoard) profiler to profile training and evaluation steps. Note To view the traces, ensure TensorBoard is installed. Then run tensorboard --logdir=<log_dir> . See https://jax.readthedocs.io/en/latest/profiling.html for more information. Source code in solstice/trainer.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 class ProfilingCallback ( Callback ): \"\"\"Uses the built-in JAX (TensorBoard) profiler to profile training and evaluation steps. !!! note To view the traces, ensure TensorBoard is installed. Then run `tensorboard --logdir=<log_dir>`. See https://jax.readthedocs.io/en/latest/profiling.html for more information.\"\"\" def __init__ ( self , log_dir : str , steps_to_profile : list [ int ] | None = None ) -> None : \"\"\"Initialize the Profiler callback. !!! tip You can use the `steps_to_profile` argument to profile only a subset of the steps. Usually, step 0 will be slowest due to JIT compilation, so you might want to profile steps 0 and 1. Args: log_dir (str): Directory to write the profiler trace files to. steps_to_profile (list[int] | None, optional): If given, only profile these steps, else profile every step. Defaults to None. \"\"\" self . log_dir = log_dir self . steps_to_profile = steps_to_profile def on_step_start ( self , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch , ) -> None : del exp , mode , batch if self . steps_to_profile is None or global_step in self . steps_to_profile : jax . profiler . start_trace ( self . log_dir ) def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch , ) -> None : del exp , mode , batch , outs if self . steps_to_profile is None or global_step in self . steps_to_profile : jax . profiler . stop_trace () __init__ ( log_dir : str , steps_to_profile : list [ int ] | None = None ) -> None \u00a4 Initialize the Profiler callback. Tip You can use the steps_to_profile argument to profile only a subset of the steps. Usually, step 0 will be slowest due to JIT compilation, so you might want to profile steps 0 and 1. Parameters: log_dir ( str ) \u2013 Directory to write the profiler trace files to. steps_to_profile ( list [ int ] | None ) \u2013 If given, only profile these steps, else profile every step. Defaults to None. Source code in solstice/trainer.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def __init__ ( self , log_dir : str , steps_to_profile : list [ int ] | None = None ) -> None : \"\"\"Initialize the Profiler callback. !!! tip You can use the `steps_to_profile` argument to profile only a subset of the steps. Usually, step 0 will be slowest due to JIT compilation, so you might want to profile steps 0 and 1. Args: log_dir (str): Directory to write the profiler trace files to. steps_to_profile (list[int] | None, optional): If given, only profile these steps, else profile every step. Defaults to None. \"\"\" self . log_dir = log_dir self . steps_to_profile = steps_to_profile EarlyStoppingCallback \u00a4 Bases: Callback Stops training early if a criterion is met. Checks once per validation epoch (at the end). This callback accumulates auxiliary outputs from each validation step into a list and passes them to the criterion function which determines whether to stop training. Tip If this callback doesn't suit your needs, you can implement your own early stopping callback by raising an EarlyStoppingException in the on_step_end hook. Source code in solstice/trainer.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class EarlyStoppingCallback ( Callback ): \"\"\"Stops training early if a criterion is met. Checks once per validation epoch (at the end). This callback accumulates auxiliary outputs from each validation step into a list and passes them to the criterion function which determines whether to stop training. !!! tip If this callback doesn't suit your needs, you can implement your own early stopping callback by raising an `EarlyStoppingException` in the `on_step_end` hook. \"\"\" def __init__ ( self , criterion_fn : Callable [[ list [ Any ]], bool ], accumulate_every_n_steps : int = 1 , ) -> None : \"\"\"Initialize the EarlyStoppingCallback. Args: criterion_fn (Callable[[list[Any]], bool]): Function that takes a list of the accumulated auxiliary outputs from each step and returns a boolean indicating whether to stop training. accumulate_every_n_steps (int, optional): Accumulate auxiliary outputs every nth step. Set to 2 to only keep half, 3 for keeping 1/3, etc. This effectively downsamples the signal (so beware it is losing information). Defaults to 1. !!! example Example criterion function takes the final metrics object, calls .compute() on it to return a dictionary, and stops training if accuracy is > 0.9: TODO: update example when `solstice.reduce` is implemented ```python criterion fn = lambda metrics: metrics.compute()[\"accuracy\"] > 0.9 ``` \"\"\" self . accumulated_outs = [] self . criterion_fn = criterion_fn self . accumulate_every_n_steps = accumulate_every_n_steps def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch , ) -> None : del exp , batch if mode == \"val\" and global_step % self . accumulate_every_n_steps == 0 : self . accumulated_outs . append ( outs ) def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : del exp , epoch if mode == \"val\" : if self . criterion_fn ( self . accumulated_outs ): raise EarlyStoppingException () self . accumulated_outs = [] # reset for next epoch __init__ ( criterion_fn : Callable [[ list [ Any ]], bool ], accumulate_every_n_steps : int = 1 ) -> None \u00a4 Initialize the EarlyStoppingCallback. Parameters: criterion_fn ( Callable [[ list [ Any ]], bool ] ) \u2013 Function that takes a list of the accumulated auxiliary outputs from each step and returns a boolean indicating whether to stop training. accumulate_every_n_steps ( int ) \u2013 Accumulate auxiliary outputs every nth step. Set to 2 to only keep half, 3 for keeping 1/3, etc. This effectively downsamples the signal (so beware it is losing information). Defaults to 1. Example Example criterion function takes the final metrics object, calls .compute() on it to return a dictionary, and stops training if accuracy is > 0.9: TODO: update example when solstice.reduce is implemented criterion fn = lambda metrics : metrics . compute ()[ \"accuracy\" ] > 0.9 Source code in solstice/trainer.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def __init__ ( self , criterion_fn : Callable [[ list [ Any ]], bool ], accumulate_every_n_steps : int = 1 , ) -> None : \"\"\"Initialize the EarlyStoppingCallback. Args: criterion_fn (Callable[[list[Any]], bool]): Function that takes a list of the accumulated auxiliary outputs from each step and returns a boolean indicating whether to stop training. accumulate_every_n_steps (int, optional): Accumulate auxiliary outputs every nth step. Set to 2 to only keep half, 3 for keeping 1/3, etc. This effectively downsamples the signal (so beware it is losing information). Defaults to 1. !!! example Example criterion function takes the final metrics object, calls .compute() on it to return a dictionary, and stops training if accuracy is > 0.9: TODO: update example when `solstice.reduce` is implemented ```python criterion fn = lambda metrics: metrics.compute()[\"accuracy\"] > 0.9 ``` \"\"\" self . accumulated_outs = [] self . criterion_fn = criterion_fn self . accumulate_every_n_steps = accumulate_every_n_steps train ( exp : ExperimentType , num_epochs : int , train_ds : tf . data . Dataset , val_ds : tf . data . Dataset | None = None , callbacks : list [ Callback ] | None = None ) -> ExperimentType \u00a4 Train a solstice.Experiment , using tf.data.Dataset for data loading. Supply solstice.Callback s to add any additional functionality. Parameters: exp ( Experiment ) \u2013 Solstice experiment to train. num_epochs ( int ) \u2013 Number of epochs to train for. train_ds ( tf . data . Dataset ) \u2013 TensorFlow dataset of training data. val_ds ( tf . data . Dataset | None ) \u2013 TensorFlow dataset of validation data. If none is given, validation is skipped. Defaults to None. callbacks ( list [ Callback ] | None ) \u2013 List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging and checkpointing. See solstice.Callback . Defaults to None. Returns: Experiment ( ExperimentType ) \u2013 Trained experiment. Source code in solstice/trainer.py 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 def train ( exp : ExperimentType , num_epochs : int , train_ds : tf . data . Dataset , val_ds : tf . data . Dataset | None = None , callbacks : list [ Callback ] | None = None , ) -> ExperimentType : \"\"\"Train a `solstice.Experiment`, using `tf.data.Dataset` for data loading. Supply `solstice.Callback`s to add any additional functionality. Args: exp (Experiment): Solstice experiment to train. num_epochs (int): Number of epochs to train for. train_ds (tf.data.Dataset): TensorFlow dataset of training data. val_ds (tf.data.Dataset | None, optional): TensorFlow dataset of validation data. If none is given, validation is skipped. Defaults to None. callbacks (list[Callback] | None, optional): List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging and checkpointing. See `solstice.Callback`. Defaults to None. Returns: Experiment: Trained experiment. \"\"\" for epoch in tqdm ( range ( num_epochs ), desc = \"Training\" , unit = \"epoch\" ): assert isinstance ( epoch , int ) # just for mypy for type narrowing for mode , ds in zip ([ \"train\" , \"val\" ], [ train_ds , val_ds ]): assert _is_valid_mode ( mode ) # type narrowing if ds is None : continue [ cb . on_epoch_start ( exp , epoch , mode ) for cb in callbacks ] if callbacks is not None else None global_step = epoch * len ( ds ) # nb: separate step counts for train and val for batch in tqdm ( ds . as_numpy_iterator (), total = len ( ds ), desc = f \" { mode } \" , leave = False , unit = \"step\" , ): global_step += 1 [ cb . on_step_start ( exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None exp , outs = ( exp . train_step ( batch ) if mode == \"train\" else exp . eval_step ( batch ) ) [ cb . on_step_end ( outs , exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None try : [ cb . on_epoch_end ( exp , epoch , mode ) for cb in callbacks ] if callbacks is not None else None except EarlyStoppingException : logging . info ( f \"Early stopping at epoch { epoch } \" ) return exp return exp test ( exp : Experiment , test_ds : tf . data . Dataset , callbacks : list [ Callback ] | None = None , return_outs : bool = False ) -> list [ Any ] | None \u00a4 Test a solstice.Experiment , using tf.data.Dataset for data loading. Supply solstice.Callback s to add any additional functionality. Parameters: exp ( Experiment ) \u2013 Experiment to test. test_ds ( tf . data . Dataset ) \u2013 TensorFlow dataset of test data. callbacks ( list [ Callback ] | None ) \u2013 List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging. See solstice.Callback . Defaults to None. return_outs ( bool ) \u2013 If True, the auxiliary outputs from exp.eval_step() are accumulated into a list and returned, else this function returns nothing. Defaults to False. Tip Testing simply involves running through the test_ds for a single epoch. Thus the on_epoch_start() and on_epoch_end() callback hooks are executed once each, before testing starts and after testing ends. Returns: list [ Any ] | None \u2013 list[Any] | None: List of auxiliary outputs from exp.eval_step() if return_outs is True, else None. Source code in solstice/trainer.py 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def test ( exp : Experiment , test_ds : tf . data . Dataset , callbacks : list [ Callback ] | None = None , return_outs : bool = False , ) -> list [ Any ] | None : \"\"\"Test a `solstice.Experiment`, using `tf.data.Dataset` for data loading. Supply `solstice.Callback`s to add any additional functionality. Args: exp (Experiment): Experiment to test. test_ds (tf.data.Dataset): TensorFlow dataset of test data. callbacks (list[Callback] | None, optional): List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging. See `solstice.Callback`. Defaults to None. return_outs (bool, optional): If True, the auxiliary outputs from `exp.eval_step()` are accumulated into a list and returned, else this function returns nothing. Defaults to False. !!! tip Testing simply involves running through the test_ds for a single epoch. Thus the `on_epoch_start()` and `on_epoch_end()` callback hooks are executed once each, before testing starts and after testing ends. Returns: list[Any] | None: List of auxiliary outputs from `exp.eval_step()` if return_outs is True, else None. \"\"\" assert callbacks is not None or return_outs is True , ( \"No callbacks were provided and return_outs is False. This function thus has no\" \" return vaules or side effects. All it does is heat up the planet :(\" ) mode : Literal [ \"test\" ] = \"test\" [ cb . on_epoch_start ( exp , 0 , mode ) for cb in callbacks ] if callbacks is not None else None global_step = 0 outputs_list = [] for batch in tqdm ( test_ds . as_numpy_iterator (), total = len ( test_ds ), desc = \"Testing\" , unit = \"step\" ): global_step += 1 [ cb . on_step_start ( exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None exp , outs = exp . eval_step ( batch ) outputs_list . append ( outs ) if return_outs else None [ cb . on_step_end ( outs , exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None [ cb . on_epoch_end ( exp , 0 , mode ) for cb in callbacks ] if callbacks is not None else None return outputs_list if return_outs else None Utilities \u00a4 Miscellaneous utilities for Solstice. EarlyStoppingException \u00a4 Bases: Exception A callback can raise this exception on_epoch_end to break the training loop early. Useful if you want to write a custom alternative to EarlyStoppingCallback . Source code in solstice/utils.py 52 53 54 55 56 class EarlyStoppingException ( Exception ): \"\"\"A callback can raise this exception `on_epoch_end` to break the training loop early. Useful if you want to write a custom alternative to `EarlyStoppingCallback`.\"\"\" pass replace ( obj : Module , ** changes : Any ) -> Module \u00a4 Make out-of-place changes to a Module, returning a new module with changes applied. Just a wrapper around equinox.tree_at . Example You can use this in the same way as dataclasses.replace , but it only works with eqx.Module s. The advantage is that it can be used when custom __init__ constructors are defined. For more info, see https://github.com/patrick-kidger/equinox/issues/120 . import equinox as eqx import solstice class Counter ( eqx . Module ): x : int def __init__ ( self , z : int ): # 'smart' constructor inits x by calculating from z self . x = 2 * z def increment ( self ): return solstice . replace ( self , x = self . x + 1 ) C1 = Counter ( z = 0 ) assert C1 . x == 0 C2 = C1 . increment () assert C2 . x == 1 Parameters: obj ( Module ) \u2013 Module to make changes to (subclass of eqx.Module ). **changes ( Any ) \u2013 Keyword arguments to replace in the module. Returns: Module ( Module ) \u2013 New instance of obj with the changes applied. Source code in solstice/utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def replace ( obj : Module , ** changes : Any ) -> Module : \"\"\"Make out-of-place changes to a Module, returning a new module with changes applied. Just a wrapper around `equinox.tree_at`. !!! example You can use this in the same way as `dataclasses.replace`, but it only works with `eqx.Module`s. The advantage is that it can be used when custom `__init__` constructors are defined. For more info, see https://github.com/patrick-kidger/equinox/issues/120. ```python import equinox as eqx import solstice class Counter(eqx.Module): x: int def __init__(self, z: int): # 'smart' constructor inits x by calculating from z self.x = 2 * z def increment(self): return solstice.replace(self, x=self.x+1) C1 = Counter(z=0) assert C1.x == 0 C2 = C1.increment() assert C2.x == 1 ``` Args: obj (Module): Module to make changes to (subclass of `eqx.Module`). **changes (Any): Keyword arguments to replace in the module. Returns: Module: New instance of `obj` with the changes applied. \"\"\" keys , vals = zip ( * changes . items ()) return eqx . tree_at ( lambda c : [ getattr ( c , key ) for key in keys ], obj , vals ) # type: ignore","title":"solstice"},{"location":"api/solstice/#whole-api","text":"Abstract This is all of Solstice. Everything is accessible through the solstice.* namespace.","title":"Whole API"},{"location":"api/solstice/#experiments","text":"The Experiment is at the heart of Solstice. The API is similar to the pl.LightningModule loved by PyTorch-Lightning users, but we do less 'magic' to keep it as transparent as possible. If in doubt, just read the source code - it's really short!","title":"Experiments"},{"location":"api/solstice/#solstice.experiment.Experiment","text":"Bases: eqx . Module , ABC Base class for Solstice experiments. An Experiment holds all stateful models, optimizers, etc... for a run and implements this interface. To make your own experiments, subclass this class and implement the logic for initialisation, training, and evaluating. Tip This is a subclass of equinox.Module , so you are free to use pure JAX transformations such as jax.jit and jax.pmap , as long as you remember to filter out static PyTree fields (e.g. with eqx.filter_jit ). Example Pseudocode for typical Experiment usage: exp = MyExperiment ( ... ) # initialise experiment state for step in range ( num_steps ): exp , outs = exp . train_step ( batch ) #do anything with the outputs here # exp is just a pytree, so we can save and restore checkpoints like so... equinox . tree_serialise_leaves ( \"checkpoint_0.eqx\" , exp ) This class just specifies a recommended interface for experiment code. Experiments implementing this interface will automatically work with the Solstice training loops. You can always create or override methods as you wish and no methods are special-cased. For example it is common to define a __call__ method to perform inference on a batch of data. Source code in solstice/experiment.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class Experiment ( eqx . Module , ABC ): \"\"\"Base class for Solstice experiments. An Experiment holds all stateful models, optimizers, etc... for a run and implements this interface. To make your own experiments, subclass this class and implement the logic for initialisation, training, and evaluating. !!! tip This is a subclass of `equinox.Module`, so you are free to use pure JAX transformations such as `jax.jit` and `jax.pmap`, as long as you remember to filter out static PyTree fields (e.g. with `eqx.filter_jit`). !!! example Pseudocode for typical `Experiment` usage: ```python exp = MyExperiment(...) # initialise experiment state for step in range(num_steps): exp, outs = exp.train_step(batch) #do anything with the outputs here # exp is just a pytree, so we can save and restore checkpoints like so... equinox.tree_serialise_leaves(\"checkpoint_0.eqx\", exp) ``` This class just specifies a recommended interface for experiment code. Experiments implementing this interface will automatically work with the Solstice training loops. You can always create or override methods as you wish and no methods are special-cased. For example it is common to define a `__call__` method to perform inference on a batch of data. \"\"\" @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialise the experiment. !!! example Pseudocode implementation for initialising an MNIST classifier with flax and optax: ```python class MNISTExperiment(Experiment): params: Any opt_state: Any opt_apply: Callable model_apply: Callable num_classes: int def __init__(self, rng: int, model: flax.nn.Module, optimizer = optax.GradientTransformation ) -> None: key = jax.random.PRNGKey(rng) dummy_batch = jnp.zeros((32, 784)) self.params = model.init(key, dummy_batch) self.model_apply = model.apply self.opt = optax.adam(learning_rate=1e-3) self.opt_state = optimizer.init(self.params) self.num_classes = 10 ``` \"\"\" raise NotImplementedError () @abstractmethod def train_step ( self , batch : Any ) -> Tuple [ Experiment , Any ]: \"\"\"A training step takes a batch of data and returns the updated experiment and any auxiliary outputs (usually a `solstice.Metrics` object). !!! tip You will typically want to use `jax.jit`, `jax.pmap`, `eqx.filter_jit`, or `eqx.filter_pmap` on this method. See the [solstice primer](https://charl-ai.github.io/Solstice/primer/) for more info on filtered transformations. You can also read the tutorial on different [parallelism strategies](https://charl-ai.github.io/Solstice/parallelism_strategies/). !!! example Pseudocode implementation of a training step: ```python class MNISTExperiment(Experiment): @eqx.filter_jit(kwargs=dict(batch=True)) def train_step(self, batch: Tuple[np.ndarray, ...] ) -> Tuple[Experiment, solstice.Metrics]: imgs, labels = batch def loss_fn(params, x, y): ... # compute loss return loss, logits (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)( self.params, imgs, labels ) new_params, new_opt_state = ... # calculate grads and update params preds = jnp.argmax(logits, axis=-1) metrics = MyMetrics(preds, labels, loss) return ( solstice.replace(self, params=new_params, opt_state=new_opt_state), metrics, ) ``` !!! tip You can use the `solstice.replace` function as a way of returning an experiment instance with modified state. Args: batch (Any): Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. \"\"\" raise NotImplementedError () @abstractmethod def eval_step ( self , batch : Any ) -> Tuple [ Experiment , Any ]: \"\"\"An evaluation step (e.g. for validation or testing) takes a batch of data and returns the updated experiment and any auxiliary outputs. Usually, this will be a `solstice.Metrics` object. Like `train_step()`, you should probably JIT this method. !!! tip In most evaluation cases, the experiment returned will be unchanged, the main reason why you would want to modify it is to advance PRNG state. !!! example Pseudocode implementation of an evaluation step: ```python class MNISTExperiment(Experiment): @eqx.filter_jit(kwargs=dict(batch=True)) def eval_step(self, batch: Tuple[np.ndarray, ...] ) -> Tuple[Experiment, Any]: imgs, labels = batch logits = ... # apply the model e.g. self.apply_fn(imgs) loss = ... # compute loss preds = jnp.argmax(logits, axis=-1) metrics = MyMetrics(preds, labels, loss) return self, metrics ``` Args: batch (Any): Batch of data. Usually, this will be either a tuple of (input, target) arrays or a dictionary mapping keys to arrays. Returns: Tuple[Experiment, Any]: A new instance of the Experiment with the updated state and any auxiliary outputs, such as metrics. \"\"\" raise NotImplementedError ()","title":"Experiment"},{"location":"api/solstice/#metrics","text":"Our Metrics API is similar to the one in CLU , although more sexy because we use equinox :) We favour defining one single object for handling all metrics for an experiment instead of composing multiple objects into a collection. This is more efficient because often we can calculate a battery of metrics from the same intermediate results. It is also simpler and easier to reason about.","title":"Metrics"},{"location":"api/solstice/#solstice.metrics.Metrics","text":"Bases: eqx . Module , ABC Base class for metrics. A Metrics object handles calculating intermediate metrics from model outputs, accumulating them over batches, then calculating final metrics from accumulated metrics. Subclass this class and implement the interface for initialisation, accumulation, and finalisation. Tip This class doesn't have to handle 'metrics' in the strictest sense. You could implement a Metrics class to collect output images for plotting for example. Example Pseudocode for typical Metrics usage: metrics = None for batch in dataset : batch_metrics = step ( batch ) # step returns a Metrics object metrics = metrics . merge ( batch_metrics ) if metrics else batch_metrics if time_to_log : metrics_dict = metrics . compute () ... # log your metrics here metrics = None # reset the object Source code in solstice/metrics.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 class Metrics ( eqx . Module , ABC ): \"\"\"Base class for metrics. A Metrics object handles calculating intermediate metrics from model outputs, accumulating them over batches, then calculating final metrics from accumulated metrics. Subclass this class and implement the interface for initialisation, accumulation, and finalisation. !!! tip This class doesn't have to handle 'metrics' in the strictest sense. You could implement a `Metrics` class to collect output images for plotting for example. !!! example Pseudocode for typical `Metrics` usage: ```python metrics = None for batch in dataset: batch_metrics = step(batch) # step returns a Metrics object metrics = metrics.merge(batch_metrics) if metrics else batch_metrics if time_to_log: metrics_dict = metrics.compute() ... # log your metrics here metrics = None # reset the object ``` \"\"\" @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialise a metrics object, typically with predictions and targets. !!! example Pseudocode for typical `Metrics` initialisation, this example object will keep track of the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): count: int num_correct: int def __init__(self, preds: jnp.ndarray, targets: jnp.ndarray) -> None: self.count = preds.shape[0] # assumes batch is first dim self.num_correct = jnp.sum(preds == targets) ``` !!! tip In classification settings, the confusion matrix is a useful intermediate result to calculate during initialisation. \"\"\" raise NotImplementedError @abstractmethod def merge ( self , other : Metrics ) -> Metrics : \"\"\"Merge two metrics objects, returning a new metrics object. !!! example Pseudocode for typical `Metrics` merging, in the example code, we can simply sum the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): def merge(self, other: Metrics) -> Metrics: new_num_correct = self.num_correct + other.num_correct new_count = self.count + other.count return solstice.replace(self, num_correct=new_num_correct, count=new_count) ``` \"\"\" raise NotImplementedError @abstractmethod def compute ( self ) -> Any : \"\"\"Compute final metrics from accumulated metrics. !!! example Pseudocode for typical `Metrics` finalisation, here we calculate accuracy from the number of correct predictions and the total number of predictions: ```python class MyMetrics(Metrics): def compute(self) -> Mapping[str, float]: return {'accuracy': self.num_correct / self.count} ``` !!! tip Typically, you will want to return a dictionary of metrics. Try to put any expensive computations here, not in `__init__`. \"\"\" raise NotImplementedError","title":"Metrics"},{"location":"api/solstice/#solstice.metrics.ClassificationMetrics","text":"Bases: Metrics Basic metrics for multiclass classification tasks. Metrics included: Average Loss Accuracy Prevalence F1 score Sensitivity (TPR, recall) Positive predictive value (PPV, precision) Accuracy is reported as Top-1 accuracy which is equal to the micro-average of precision/recall/f1. Prevalence is reported on a per-class basis. Precision, Recall and F1 are reported three times: per-class, macro-average, and weighted average (by prevalence). Not for multi-label classification. Info See https://en.wikipedia.org/wiki/Confusion_matrix for more on confusion matrices and classification metrics. See https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel for more on multiclass micro/macro/weighted averaging. Source code in solstice/metrics.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 class ClassificationMetrics ( Metrics ): \"\"\"Basic metrics for multiclass classification tasks. !!! summary \"Metrics included:\" - Average Loss - Accuracy - Prevalence - F1 score - Sensitivity (TPR, recall) - Positive predictive value (PPV, precision) Accuracy is reported as Top-1 accuracy which is equal to the micro-average of precision/recall/f1. Prevalence is reported on a per-class basis. Precision, Recall and F1 are reported three times: per-class, macro-average, and weighted average (by prevalence). *Not* for multi-label classification. !!! info See https://en.wikipedia.org/wiki/Confusion_matrix for more on confusion matrices and classification metrics. See https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel for more on multiclass micro/macro/weighted averaging. \"\"\" _confusion_matrix : jnp . ndarray _average_loss : float _count : int _num_classes : int def __init__ ( self , preds : jnp . ndarray , targets : jnp . ndarray , loss : float , num_classes : int ) -> None : \"\"\" Create a ClassificationMetrics object from model predictions and targets. Args: preds (jnp.ndarray): Non OH encoded predictions, shape: (batch_size,). targets (jnp.ndarray): Non OH encoded targets, shape: (batch_size,). loss (float): Average loss over the batch (scalar). num_classes (int): Number of classes in classification problem. \"\"\" self . _confusion_matrix = _compute_confusion_matrix ( preds , targets , num_classes ) self . _average_loss = loss self . _count = preds . shape [ 0 ] self . _num_classes = num_classes def merge ( self , other : ClassificationMetrics ) -> ClassificationMetrics : assert isinstance ( other , ClassificationMetrics ), ( \"Can only merge ClassificationMetrics object with another\" f \" ClassificationMetrics object, got { type ( other ) } \" ) assert self . _num_classes == other . _num_classes , ( \"Can only merge metrics with same num_classes, got\" f \" { self . _num_classes } and { other . _num_classes } \" ) # can simply sum confusion matrices and count new_cm = self . _confusion_matrix + other . _confusion_matrix new_count = self . _count + other . _count # average loss is weighted by count from each object new_loss = ( self . _average_loss * self . _count + other . _average_loss * other . _count ) / ( self . _count + other . _count ) return replace ( self , _confusion_matrix = new_cm , _average_loss = new_loss , _count = new_count ) def compute ( self ) -> Mapping [ str , float ]: metrics = _compute_metrics_from_cm ( self . _confusion_matrix ) metrics [ \"average_loss\" ] = self . _average_loss return metrics","title":"ClassificationMetrics"},{"location":"api/solstice/#training","text":"Training loops are usually boilerplate code that has little to do with your research. We provide training and testing loops which integrate with a simple and flexible callback system. Any solstice.Experiment can be passed to the loops, but you can always write your own if necessary. We provide a handful of pre-implemented callbacks, but if they do not suit your needs, you can use them as inspiration to write your own.","title":"Training"},{"location":"api/solstice/#solstice.trainer.Callback","text":"Bases: ABC Base class for callbacks to solstice.train() and `solstice.test(). Subclass and implement this interface to inject arbitrary functionality into the training and testing loops. Tip All callback hooks return None , so they cannot affect the training itself. Use callbacks to execute side effects like logging, checkpointing or profiling. Example Pseudocode callback implementation for logging with solstice.Metrics : class MyLoggingCallback ( Callback ): def __init__ ( self , log_every_n_steps , ... ): self . metrics = None self . log_every_n_steps = log_every_n_steps ... # set up logging, e.g. wandb.init(...) def on_step_end ( self , exp , global_step , training , batch , outs ): assert isinstance ( outs , solstice . Metrics ) self . metrics = outs . merge ( self . metrics ) if self . metrics else outs if ( global_step + 1 ) % self . log_every_n_steps == 0 : metrics_dict = self . metrics . compute () ... # do logging e.g. wandb.log(metrics_dict) self . metrics = None Source code in solstice/trainer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class Callback ( ABC ): \"\"\"Base class for callbacks to `solstice.train()` and `solstice.test(). Subclass and implement this interface to inject arbitrary functionality into the training and testing loops. !!! tip All callback hooks return `None`, so they cannot affect the training itself. Use callbacks to execute side effects like logging, checkpointing or profiling. !!! example Pseudocode callback implementation for logging with `solstice.Metrics`: ```python class MyLoggingCallback(Callback): def __init__(self, log_every_n_steps, ...): self.metrics = None self.log_every_n_steps = log_every_n_steps ... # set up logging, e.g. wandb.init(...) def on_step_end(self, exp, global_step, training, batch, outs): assert isinstance(outs, solstice.Metrics) self.metrics = outs.merge(self.metrics) if self.metrics else outs if (global_step + 1) % self.log_every_n_steps == 0: metrics_dict = self.metrics.compute() ... # do logging e.g. wandb.log(metrics_dict) self.metrics = None ``` \"\"\" @abstractmethod def __init__ ( self , * args , ** kwargs ) -> None : \"\"\"Initialize the callback.\"\"\" raise NotImplementedError def on_epoch_start ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : \"\"\"Called at the start of each epoch, i.e. before the model has seen any data for that epoch. Args: exp (Experiment): Current Experiment state. epoch (int): Current epoch number. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing epoch. \"\"\" pass def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : \"\"\"Called at the end of each epoch, i.e. after the model has seen the full dataset for that epoch. Args: exp (Experiment): Current Experiment state. epoch (int): Current epoch number. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. \"\"\" pass def on_step_start ( self , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : \"\"\"Called at the start of each training and validation step, i.e. before the batch has been seen. Args: exp (Experiment): Current Experiment state. global_step (int): Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. batch (Any): Current batch of data for this step. \"\"\" pass def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : \"\"\"Called at the end of each training and validation step, i.e. after the batch has been seen. Args: exp (Experiment): Current Experiment state. global_step (int): Current step number. This is the global step, i.e. the total number of training or validation or testing steps seen so far. Note that we keep separate step counts for training and validation, so it might not be unique. mode (Literal[\"train\", \"val\", \"test\"]): String representing whether this is a training, validation or testing step. batch (Any): Current batch of data for this step. outs (Any): Auxiliary outputs from the experiment train/eval step. Usually, this should be a `solstice.Metrics` object. \"\"\" pass","title":"Callback"},{"location":"api/solstice/#solstice.trainer.LoggingCallback","text":"Bases: Callback Logs auxiliary outputs from training or evaulation steps (either periodically every n steps, or at the end of the epoch). Internally, this accumulates metrics with metrics.merge() , computes them with metrics.compute() , and then passes the final results to the given logging function. Warning Auxiliary outputs from the train and eval steps must be a solstice.Metrics instance for this callback to work properly. We raise an AssertionError if this is not the case. Note There are many different libraries you can use for writing logs (e.g. wandb, TensorBoard(X), ...). We offer no opinion on which one you should use. Pass in a logging function to use any arbitrary logger. Source code in solstice/trainer.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 class LoggingCallback ( Callback ): \"\"\"Logs auxiliary outputs from training or evaulation steps (either periodically every n steps, or at the end of the epoch). Internally, this accumulates metrics with `metrics.merge()`, computes them with `metrics.compute()`, and then passes the final results to the given logging function. !!! warning Auxiliary outputs from the train and eval steps must be a `solstice.Metrics` instance for this callback to work properly. We raise an AssertionError if this is not the case. !!! note There are many different libraries you can use for writing logs (e.g. wandb, TensorBoard(X), ...). We offer no opinion on which one you should use. Pass in a logging function to use any arbitrary logger. \"\"\" def __init__ ( self , log_every_n_steps : int | None = None , logging_fn : Callable [[ Any , int , Literal [ \"train\" , \"val\" , \"test\" ]], None ] | None = None , ) -> None : \"\"\"Initialize the logging callback. Args: log_every_n_steps (int | None, optional): If given, accumulate metrics over n steps before logging. If None, log at end of epoch. Defaults to None. logging_fn (Callable[[Any, int, Literal['train', 'val', 'test']], None] | None, optional): Logging function. Takes the outputs of `metrics.compute()`, the current step or epoch number, and a string representing whether training, validating, or testing. The function should return nothing. If no logging_fn is given, the default behaviour is to log with the built in Python logger (INFO level). Defaults to None. !!! example The default logging function (used if None is given) logs using the built in Python logger, with name \"solstice\" and INFO level (notice that the output of `metrics.compute()` must be printable): ```python logger = logging.getLogger(\"solstice\") default_logger = lambda metrics, step, mode: logging.info( f\"{mode} step {step}: {metrics}\" ) ``` If the logs aren't showing, you might need to put this line at the top of your script: ```python import logging logging.getLogger(\"solstice\").setLevel(logging.INFO) ``` \"\"\" default_logger = lambda metrics , step , mode : logger . info ( f \" { mode } step { step } : { metrics } \" ) self . logging_fn = logging_fn if logging_fn else default_logger self . log_every_n_steps = log_every_n_steps self . metrics = None def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch : Any , ) -> None : del exp , batch assert isinstance ( outs , Metrics ) self . metrics = outs . merge ( self . metrics ) if self . metrics else outs if self . log_every_n_steps and ( global_step + 1 ) % self . log_every_n_steps == 0 : final_metrics = self . metrics . compute () self . logging_fn ( final_metrics , global_step , mode ) self . metrics = None def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : del exp # if not logging every n steps, we just log at the end of the epoch if not self . log_every_n_steps : assert self . metrics is not None final_metrics = self . metrics . compute () self . logging_fn ( final_metrics , epoch , mode ) # reset the metrics object to prevent train/val metrics from being mixed self . metrics = None","title":"LoggingCallback"},{"location":"api/solstice/#solstice.trainer.CheckpointingCallback","text":"Bases: Callback Checkpoint the experiment state at the end of each epoch. Todo Implement this. Consider adding asynchronous checkpointing. Source code in solstice/trainer.py 231 232 233 234 235 236 237 class CheckpointingCallback ( Callback ): \"\"\"Checkpoint the experiment state at the end of each epoch. !!! todo Implement this. Consider adding asynchronous checkpointing.\"\"\" pass","title":"CheckpointingCallback"},{"location":"api/solstice/#solstice.trainer.ProfilingCallback","text":"Bases: Callback Uses the built-in JAX (TensorBoard) profiler to profile training and evaluation steps. Note To view the traces, ensure TensorBoard is installed. Then run tensorboard --logdir=<log_dir> . See https://jax.readthedocs.io/en/latest/profiling.html for more information. Source code in solstice/trainer.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 class ProfilingCallback ( Callback ): \"\"\"Uses the built-in JAX (TensorBoard) profiler to profile training and evaluation steps. !!! note To view the traces, ensure TensorBoard is installed. Then run `tensorboard --logdir=<log_dir>`. See https://jax.readthedocs.io/en/latest/profiling.html for more information.\"\"\" def __init__ ( self , log_dir : str , steps_to_profile : list [ int ] | None = None ) -> None : \"\"\"Initialize the Profiler callback. !!! tip You can use the `steps_to_profile` argument to profile only a subset of the steps. Usually, step 0 will be slowest due to JIT compilation, so you might want to profile steps 0 and 1. Args: log_dir (str): Directory to write the profiler trace files to. steps_to_profile (list[int] | None, optional): If given, only profile these steps, else profile every step. Defaults to None. \"\"\" self . log_dir = log_dir self . steps_to_profile = steps_to_profile def on_step_start ( self , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch , ) -> None : del exp , mode , batch if self . steps_to_profile is None or global_step in self . steps_to_profile : jax . profiler . start_trace ( self . log_dir ) def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch , ) -> None : del exp , mode , batch , outs if self . steps_to_profile is None or global_step in self . steps_to_profile : jax . profiler . stop_trace ()","title":"ProfilingCallback"},{"location":"api/solstice/#solstice.trainer.EarlyStoppingCallback","text":"Bases: Callback Stops training early if a criterion is met. Checks once per validation epoch (at the end). This callback accumulates auxiliary outputs from each validation step into a list and passes them to the criterion function which determines whether to stop training. Tip If this callback doesn't suit your needs, you can implement your own early stopping callback by raising an EarlyStoppingException in the on_step_end hook. Source code in solstice/trainer.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class EarlyStoppingCallback ( Callback ): \"\"\"Stops training early if a criterion is met. Checks once per validation epoch (at the end). This callback accumulates auxiliary outputs from each validation step into a list and passes them to the criterion function which determines whether to stop training. !!! tip If this callback doesn't suit your needs, you can implement your own early stopping callback by raising an `EarlyStoppingException` in the `on_step_end` hook. \"\"\" def __init__ ( self , criterion_fn : Callable [[ list [ Any ]], bool ], accumulate_every_n_steps : int = 1 , ) -> None : \"\"\"Initialize the EarlyStoppingCallback. Args: criterion_fn (Callable[[list[Any]], bool]): Function that takes a list of the accumulated auxiliary outputs from each step and returns a boolean indicating whether to stop training. accumulate_every_n_steps (int, optional): Accumulate auxiliary outputs every nth step. Set to 2 to only keep half, 3 for keeping 1/3, etc. This effectively downsamples the signal (so beware it is losing information). Defaults to 1. !!! example Example criterion function takes the final metrics object, calls .compute() on it to return a dictionary, and stops training if accuracy is > 0.9: TODO: update example when `solstice.reduce` is implemented ```python criterion fn = lambda metrics: metrics.compute()[\"accuracy\"] > 0.9 ``` \"\"\" self . accumulated_outs = [] self . criterion_fn = criterion_fn self . accumulate_every_n_steps = accumulate_every_n_steps def on_step_end ( self , outs : Any , exp : Experiment , global_step : int , mode : Literal [ \"train\" , \"val\" , \"test\" ], batch , ) -> None : del exp , batch if mode == \"val\" and global_step % self . accumulate_every_n_steps == 0 : self . accumulated_outs . append ( outs ) def on_epoch_end ( self , exp : Experiment , epoch : int , mode : Literal [ \"train\" , \"val\" , \"test\" ] ) -> None : del exp , epoch if mode == \"val\" : if self . criterion_fn ( self . accumulated_outs ): raise EarlyStoppingException () self . accumulated_outs = [] # reset for next epoch","title":"EarlyStoppingCallback"},{"location":"api/solstice/#solstice.trainer.train","text":"Train a solstice.Experiment , using tf.data.Dataset for data loading. Supply solstice.Callback s to add any additional functionality. Parameters: exp ( Experiment ) \u2013 Solstice experiment to train. num_epochs ( int ) \u2013 Number of epochs to train for. train_ds ( tf . data . Dataset ) \u2013 TensorFlow dataset of training data. val_ds ( tf . data . Dataset | None ) \u2013 TensorFlow dataset of validation data. If none is given, validation is skipped. Defaults to None. callbacks ( list [ Callback ] | None ) \u2013 List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging and checkpointing. See solstice.Callback . Defaults to None. Returns: Experiment ( ExperimentType ) \u2013 Trained experiment. Source code in solstice/trainer.py 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 def train ( exp : ExperimentType , num_epochs : int , train_ds : tf . data . Dataset , val_ds : tf . data . Dataset | None = None , callbacks : list [ Callback ] | None = None , ) -> ExperimentType : \"\"\"Train a `solstice.Experiment`, using `tf.data.Dataset` for data loading. Supply `solstice.Callback`s to add any additional functionality. Args: exp (Experiment): Solstice experiment to train. num_epochs (int): Number of epochs to train for. train_ds (tf.data.Dataset): TensorFlow dataset of training data. val_ds (tf.data.Dataset | None, optional): TensorFlow dataset of validation data. If none is given, validation is skipped. Defaults to None. callbacks (list[Callback] | None, optional): List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging and checkpointing. See `solstice.Callback`. Defaults to None. Returns: Experiment: Trained experiment. \"\"\" for epoch in tqdm ( range ( num_epochs ), desc = \"Training\" , unit = \"epoch\" ): assert isinstance ( epoch , int ) # just for mypy for type narrowing for mode , ds in zip ([ \"train\" , \"val\" ], [ train_ds , val_ds ]): assert _is_valid_mode ( mode ) # type narrowing if ds is None : continue [ cb . on_epoch_start ( exp , epoch , mode ) for cb in callbacks ] if callbacks is not None else None global_step = epoch * len ( ds ) # nb: separate step counts for train and val for batch in tqdm ( ds . as_numpy_iterator (), total = len ( ds ), desc = f \" { mode } \" , leave = False , unit = \"step\" , ): global_step += 1 [ cb . on_step_start ( exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None exp , outs = ( exp . train_step ( batch ) if mode == \"train\" else exp . eval_step ( batch ) ) [ cb . on_step_end ( outs , exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None try : [ cb . on_epoch_end ( exp , epoch , mode ) for cb in callbacks ] if callbacks is not None else None except EarlyStoppingException : logging . info ( f \"Early stopping at epoch { epoch } \" ) return exp return exp","title":"train()"},{"location":"api/solstice/#solstice.trainer.test","text":"Test a solstice.Experiment , using tf.data.Dataset for data loading. Supply solstice.Callback s to add any additional functionality. Parameters: exp ( Experiment ) \u2013 Experiment to test. test_ds ( tf . data . Dataset ) \u2013 TensorFlow dataset of test data. callbacks ( list [ Callback ] | None ) \u2013 List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging. See solstice.Callback . Defaults to None. return_outs ( bool ) \u2013 If True, the auxiliary outputs from exp.eval_step() are accumulated into a list and returned, else this function returns nothing. Defaults to False. Tip Testing simply involves running through the test_ds for a single epoch. Thus the on_epoch_start() and on_epoch_end() callback hooks are executed once each, before testing starts and after testing ends. Returns: list [ Any ] | None \u2013 list[Any] | None: List of auxiliary outputs from exp.eval_step() if return_outs is True, else None. Source code in solstice/trainer.py 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def test ( exp : Experiment , test_ds : tf . data . Dataset , callbacks : list [ Callback ] | None = None , return_outs : bool = False , ) -> list [ Any ] | None : \"\"\"Test a `solstice.Experiment`, using `tf.data.Dataset` for data loading. Supply `solstice.Callback`s to add any additional functionality. Args: exp (Experiment): Experiment to test. test_ds (tf.data.Dataset): TensorFlow dataset of test data. callbacks (list[Callback] | None, optional): List of Solstice callbacks. These can execute arbitrary code on certain events, usually for side effects like logging. See `solstice.Callback`. Defaults to None. return_outs (bool, optional): If True, the auxiliary outputs from `exp.eval_step()` are accumulated into a list and returned, else this function returns nothing. Defaults to False. !!! tip Testing simply involves running through the test_ds for a single epoch. Thus the `on_epoch_start()` and `on_epoch_end()` callback hooks are executed once each, before testing starts and after testing ends. Returns: list[Any] | None: List of auxiliary outputs from `exp.eval_step()` if return_outs is True, else None. \"\"\" assert callbacks is not None or return_outs is True , ( \"No callbacks were provided and return_outs is False. This function thus has no\" \" return vaules or side effects. All it does is heat up the planet :(\" ) mode : Literal [ \"test\" ] = \"test\" [ cb . on_epoch_start ( exp , 0 , mode ) for cb in callbacks ] if callbacks is not None else None global_step = 0 outputs_list = [] for batch in tqdm ( test_ds . as_numpy_iterator (), total = len ( test_ds ), desc = \"Testing\" , unit = \"step\" ): global_step += 1 [ cb . on_step_start ( exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None exp , outs = exp . eval_step ( batch ) outputs_list . append ( outs ) if return_outs else None [ cb . on_step_end ( outs , exp , global_step , mode , batch ) for cb in callbacks ] if callbacks is not None else None [ cb . on_epoch_end ( exp , 0 , mode ) for cb in callbacks ] if callbacks is not None else None return outputs_list if return_outs else None","title":"test()"},{"location":"api/solstice/#utilities","text":"Miscellaneous utilities for Solstice.","title":"Utilities"},{"location":"api/solstice/#solstice.utils.EarlyStoppingException","text":"Bases: Exception A callback can raise this exception on_epoch_end to break the training loop early. Useful if you want to write a custom alternative to EarlyStoppingCallback . Source code in solstice/utils.py 52 53 54 55 56 class EarlyStoppingException ( Exception ): \"\"\"A callback can raise this exception `on_epoch_end` to break the training loop early. Useful if you want to write a custom alternative to `EarlyStoppingCallback`.\"\"\" pass","title":"EarlyStoppingException"},{"location":"api/solstice/#solstice.utils.replace","text":"Make out-of-place changes to a Module, returning a new module with changes applied. Just a wrapper around equinox.tree_at . Example You can use this in the same way as dataclasses.replace , but it only works with eqx.Module s. The advantage is that it can be used when custom __init__ constructors are defined. For more info, see https://github.com/patrick-kidger/equinox/issues/120 . import equinox as eqx import solstice class Counter ( eqx . Module ): x : int def __init__ ( self , z : int ): # 'smart' constructor inits x by calculating from z self . x = 2 * z def increment ( self ): return solstice . replace ( self , x = self . x + 1 ) C1 = Counter ( z = 0 ) assert C1 . x == 0 C2 = C1 . increment () assert C2 . x == 1 Parameters: obj ( Module ) \u2013 Module to make changes to (subclass of eqx.Module ). **changes ( Any ) \u2013 Keyword arguments to replace in the module. Returns: Module ( Module ) \u2013 New instance of obj with the changes applied. Source code in solstice/utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def replace ( obj : Module , ** changes : Any ) -> Module : \"\"\"Make out-of-place changes to a Module, returning a new module with changes applied. Just a wrapper around `equinox.tree_at`. !!! example You can use this in the same way as `dataclasses.replace`, but it only works with `eqx.Module`s. The advantage is that it can be used when custom `__init__` constructors are defined. For more info, see https://github.com/patrick-kidger/equinox/issues/120. ```python import equinox as eqx import solstice class Counter(eqx.Module): x: int def __init__(self, z: int): # 'smart' constructor inits x by calculating from z self.x = 2 * z def increment(self): return solstice.replace(self, x=self.x+1) C1 = Counter(z=0) assert C1.x == 0 C2 = C1.increment() assert C2.x == 1 ``` Args: obj (Module): Module to make changes to (subclass of `eqx.Module`). **changes (Any): Keyword arguments to replace in the module. Returns: Module: New instance of `obj` with the changes applied. \"\"\" keys , vals = zip ( * changes . items ()) return eqx . tree_at ( lambda c : [ getattr ( c , key ) for key in keys ], obj , vals ) # type: ignore","title":"replace()"}]}